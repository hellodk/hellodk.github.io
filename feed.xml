<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <description>A beautiful narrative written an elegant publishing platform. The story begins here.</description>
    <link>http://hellodk.github.io///</link>
    <atom:link href="http://hellodk.github.io///feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 28 Jan 2016 03:29:17 +0530</pubDate>
    <lastBuildDate>Thu, 28 Jan 2016 03:29:17 +0530</lastBuildDate>
    <generator>Jekyll v2.4.0</generator>
    
      <item>
        <title>RethinkDb installation on Ubuntu-14.04</title>
        <description>&lt;p&gt;Let&amp;#39;s get some hands on rethinkdb today and find it out yourself.&lt;/p&gt;

&lt;p&gt;So what is rethinkdb? Well, to say...rethinkdb is an open-source, scalable JSON database built from the ground up for the realtime web. RethinkDB inverts the traditional database architecture by exposing an exciting new access model – instead of polling for changes, the developer can tell RethinkDB to continuously push updated query results to applications in realtime. RethinkDB’s realtime push architecture dramatically reduces the time and effort necessary to build scalable realtime apps. RethinkDB also offers a flexible query language, intuitive operations and monitoring APIs, and is easy to setup and learn. Just like any other database solution, rethinkdb ships as a client-server component model. The installation process for both the server and the client are illustrated below:&lt;/p&gt;

&lt;p&gt;How to Install RethinkDb:
1. Add the RethinkDB PPA to your list of repositories : source /etc/lsb-release &amp;amp;&amp;amp; echo &amp;quot;deb http://download.rethinkdb.com/apt $DISTRIB_CODENAME main&amp;quot; | sudo tee /etc/apt/sources.list.d/rethinkdb.list&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Add the keys:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;wget -qO- http://download.rethinkdb.com/apt/pubkey.gpg | sudo apt-key add -&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Update the repository:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;sudo apt-get update&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Install the rethinkdb server via apt-get:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;sudo apt-get -y install rethinkdb&lt;/p&gt;

&lt;p&gt;Install rethinkdb client:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Install the python-pip package:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;sudo apt-get install python-pip&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Install the rethinkdb python client:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;sudo pip install rethinkdb :&lt;/p&gt;

&lt;p&gt;The above steps ensure that rethinkdb is installed on the system, while it does not ensures that this will start the rethinkdb service on system startup. You still need to start the rethinkdb service using the below command&lt;/p&gt;

&lt;p&gt;The above command will ensure that rethinkdb is running as a terminal process, and will exit once the terminal is closed, or the process is killed, in short it will not run rethinkdb as a background service.&lt;/p&gt;

&lt;p&gt;To start rethinkdb as a service, please follow the below steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Go to /etc/rethinkdb and you will get the file, default.conf.sample&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Copy the file to /etc/rethinkdb/instances.d and rename the file as per your requirements ensuring the extension is .conf only. Say for example the file name is rethinkdb1.conf&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now open the file /etc/rethinkdb/instances.d/rethinkdb1.conf and modify the paramaters as per your requirements.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If setting up a cluster, I suggest do change the server-name to somethink like &amp;#39;rethinkdb-primary&amp;#39; or &amp;#39;rethinkdb-1&amp;#39; or &amp;#39;master&amp;#39; or &amp;#39;slave&amp;#39;. This will ensure that we have a meaningful naming convention for our cluster.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The default port details are :&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;29015 : Rethinkdb listens for intracluster connections&lt;/p&gt;

&lt;p&gt;28015 : Rethinkdb listens for client driver connections&lt;/p&gt;

&lt;p&gt;8080 : Rethinkdb listens for administrative HTTP connections&lt;/p&gt;

&lt;p&gt;22 : For SSH. The server uses public key authentication.&lt;/p&gt;

&lt;p&gt;80 : For HTTP. It is used during the setup process but otherwise redirects to HTTPS.&lt;/p&gt;

&lt;p&gt;443 : For HTTPS. An Nginx server sits between RethinkDB and the world and provides basic HTTP authentication and secure HTTPS connections for the web UI&lt;/p&gt;
</description>
        <pubDate>Thu, 28 Jan 2016 15:48:00 +0530</pubDate>
        <link>http://hellodk.github.io///rethinkdb_installation_on%20_ubuntu-14.04</link>
        <guid isPermaLink="true">http://hellodk.github.io///rethinkdb_installation_on%20_ubuntu-14.04</guid>
        
        <category>tutorials</category>
        
        
        <category>dk</category>
        
      </item>
    
      <item>
        <title>Getting started with Cassandra</title>
        <description>&lt;p&gt;The Growth of Big Data&lt;/p&gt;

&lt;p&gt;Big Data is one of the key forces driving the growth and popularity of NoSQL for business. The almost limitless array of data collection technologies ranging from simple online actions to point of sale systems to GPS tools to smartphones and tablets to sophisticated sensors – and many more – act as force multipliers for data growth.&lt;/p&gt;

&lt;p&gt;In fact, one of the first reasons to use NoSQL is because you have a Big Data project to tackle. A Big Data project is normally typified by:&lt;/p&gt;

&lt;p&gt;High data velocity – lots of data coming in very quickly, possibly from different locations.
Data variety – storage of data that is structured, semi-structured and unstructured.
Data volume – data that involves many terabytes or petabytes in size.
Data complexity – data that is stored and managed in different locations or data centers.&lt;/p&gt;

&lt;p&gt;Datamodel   Performance Scalability Flexibility Complexity  Functionality
Key-value store High    High    High    None    Variable (None)
Column Store    High    High    Moderate    Low Minimal
Document Store  High    Variable (High) High    Low Variable (Low)
Graph Database  Variable    Variable    High    High    Graph Theory&lt;/p&gt;

&lt;p&gt;massively scalable open source NoSQL database. Cassandra is perfect for managing large amounts of structured, semi-structured, and unstructured data across multiple data centers and the cloud. Cassandra delivers continuous availability, linear scalability, and operational simplicity across many commodity servers with no single point of failure, along with a powerful dynamic data model designed for maximum flexibility and fast response times&lt;/p&gt;

&lt;p&gt;built-for-scale architecture means that it is capable of handling petabytes of information and thousands of concurrent users/operations per second.&lt;/p&gt;

&lt;p&gt;An apache Software Foundation project, Cassandra is column oriented database and is an open source distributed database management system designed to handle large amounts of data across many commodity servers, providing high availability with no single point of failure.&lt;/p&gt;

&lt;p&gt;Cassandra does not support joins or subqueries. Rather, Cassandra emphasizes denormalization through features like collections&lt;/p&gt;

&lt;p&gt;CQL - Cassandara Query Language&lt;/p&gt;

&lt;p&gt;Each node in a cluster can accept read and write requests, regardless of where the data is actually located in the cluster.&lt;/p&gt;

&lt;p&gt;When a node goes down, read/write requests can be served from other nodes in the network.&lt;/p&gt;

&lt;p&gt;The key components of Cassandra are as follows −
Node − It is the place where data is stored.
Data center − It is a collection of related nodes.
Cluster − A cluster is a component that contains one or more data centers.
Commit log − The commit log is a crash-recovery mechanism in Cassandra. Every write operation is written to the commit log.
Mem-table − A mem-table is a memory-resident data structure. After commit log, the data will be written to the mem-table. Sometimes, for a single-column family, there will be multiple mem-tables.
SSTable − It is a disk file to which the data is flushed from the mem-table when its contents reach a threshold value.
Bloom filter − These are nothing but quick, nondeterministic, algorithms for testing whether an element is a member of a set. It is a special kind of cache. Bloom filters are accessed after every query.&lt;/p&gt;

&lt;h2&gt;Commands:&lt;/h2&gt;

&lt;p&gt;nodetool cfstats : displays statistics for each table and keyspace.
nodetool cfhistograms : provides statistics about a table, including read/write latency, row size, column count, and number of SSTables.
nodetool netstats : provides statistics about network operations and connections.
nodetool tpstats : provides statistics about the number of active, pending, and completed tasks for each stage of Cassandra operations by thread pool.
nodetool status :&lt;/p&gt;

&lt;p&gt;cqlsh &lt;machine ip&gt; -  connects to the machine cqlsh&lt;/p&gt;

&lt;h2&gt;cqlsh command list:&lt;/h2&gt;

&lt;p&gt;HELP - Displays help topics for all cqlsh commands.
CAPTURE - Captures the output of a command and adds it to a file.
CONSISTENCY - Shows the current consistency level, or sets a new consistency level.
COPY - Copies data to and from Cassandra.
DESCRIBE - Describes the current cluster of Cassandra and its objects.
EXPAND - Expands the output of a query vertically.
EXIT - Using this command, you can terminate cqlsh.
PAGING - Enables or disables query paging.
SHOW - Displays the details of current cqlsh session such as Cassandra version, host, or data type assumptions.
SOURCE - Executes a file that contains CQL statements.
TRACING - Enables or disables request tracing.&lt;/p&gt;

&lt;h2&gt;Upgrading:&lt;/h2&gt;

&lt;p&gt;mkdir ~/cassandra&lt;em&gt;backup
sudo cp -r /etc/cassandra/* ~/cassandra&lt;/em&gt;backup
sudo vi /etc/cassandra/cassandra.yaml and edit num&lt;em&gt;tokens to 1 and uncomment the initial&lt;/em&gt;token and set it to 1
nodetool upgradesstables
nodetool drain
sudo service cassandra stop
sudo cp -r /etc/cassandra/* ~/cassandra&lt;em&gt;backup&lt;/em&gt;new
sudo apt-get install cassandra=2.1.12
Open the old and new cassandra.yaml files and diff them.
Merge the diffs by hand, including the partitioner setting, from the old file into the new one.
Do not use the default partitioner setting in the new cassandra.yaml because it has changed in this release to the Murmur3Partitioner. The Murmur3Partitioner can only be used for new clusters. After data has been added to the cluster, you cannot change the partitioner without reworking tables, which is not practical. Use your old partitioner setting in the new cassandra.yaml file.
Save the file as cassandra.yaml.&lt;/p&gt;

&lt;p&gt;Configuration file &amp;#39;/etc/cassandra/cassandra.yaml&amp;#39;
 ==&amp;gt; Modified (by you or by a script) since installation.
 ==&amp;gt; Package distributor has shipped an updated version.
   What would you like to do about it ?  Your options are:
    Y or I  : install the package maintainer&amp;#39;s version
    N or O  : keep your currently-installed version
      D     : show the differences between the versions
      Z     : start a shell to examine the situation
 The default action is to keep your current version.
*** cassandra.yaml (Y/I/N/O/D/Z) [default=N] ? &lt;/p&gt;

&lt;p&gt;************&lt;strong&gt;&lt;em&gt;Inserting values into tables&lt;/em&gt;&lt;/strong&gt;**************************
CREATE KEYSPACE tutorialspoint WITH replication = {
  &amp;#39;class&amp;#39;: &amp;#39;NetworkTopologyStrategy&amp;#39;,
  &amp;#39;cdr_record&amp;#39;: &amp;#39;2&amp;#39;
};&lt;/p&gt;

&lt;p&gt;INSERT INTO tutorialspoint.emp (emp&lt;em&gt;id,emp&lt;/em&gt;city,emp&lt;em&gt;name,emp&lt;/em&gt;phone,emp_sal) VALUES(3,&amp;#39;Kolkata&amp;#39;,&amp;#39;Stag1&amp;#39;,8791134412,60);&lt;/p&gt;

&lt;p&gt;UPDATE TABLE emp(
   emp&lt;em&gt;id int PRIMARY KEY,
   emp&lt;/em&gt;name text,
   emp&lt;em&gt;city text,
   emp&lt;/em&gt;sal varint,
   emp_phone varint);&lt;/p&gt;

&lt;p&gt;INSERT INTO TABLE emp(emp&lt;em&gt;id int,emp&lt;/em&gt;name,emp&lt;em&gt;city,emp&lt;/em&gt;sal,emp_phone) VALUES(1,&amp;#39;dk&amp;#39;,&amp;#39;Bangalore&amp;#39;,24,9964014500);&lt;/p&gt;

&lt;p&gt;/usr/local/freeswitch/bin/fs_cli -x &amp;quot;originate  freetdm/8/a/009716337516 &amp;amp;park()&amp;quot;&lt;/p&gt;

&lt;h2&gt;Nodetool Command Set:&lt;/h2&gt;

&lt;p&gt;nodetool status
nodetool info
nodetool -host 10.60.8.23 ring&lt;/p&gt;
</description>
        <pubDate>Thu, 28 Jan 2016 15:48:00 +0530</pubDate>
        <link>http://hellodk.github.io///getting_familiar_with_cassandra</link>
        <guid isPermaLink="true">http://hellodk.github.io///getting_familiar_with_cassandra</guid>
        
        <category>tutorials</category>
        
        
        <category>dk</category>
        
      </item>
    
      <item>
        <title>5 interesting cloud predictions for 2016</title>
        <description>&lt;ol&gt;
&lt;li&gt;It&amp;#39;s time for IOT-&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The IOT or the Internet of Things has been a buzzword around for quite some time. And finally the time has come for IOT to be on boom. It is predicted that by the end of 2016, there will be one billion connected devices.&lt;/p&gt;

&lt;p&gt;The Internet of Things is all set to harness the awesomeness of Cloud computing this year. IOT and Cloud combined together breaks free all limitations. The duo combo can help right from analyzing the weather conditions at your home and water the plants to conducting major surgeries remotely to powering drones for military, logistics etc. and what not!!&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Cloud is expanding - AWS coming to India in 2016. Owing to the huge demand in the Indian sub-continent for Cloud services, AWS(Amazon Web Services)- one of the top cloud services provider has plans to setup India region in 2016. Do I still need to say anything more on this?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;More and more startups will be focusing towards adapting cloud culture - Cloud is so versatile and flexible, it allows you to work from any corner of the world. Startups ideally do not have the infrastructure/resources to manage their own data-centers or hardware. Cloud provides then with Infrastructure as a Service at a very affordable rates, so they can focus more on their product.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Outcast for more flexible cloud apps – The need for more flexible cloud apps can not be denied. With the rise in clod computing, will come the rise for ease of accessibility. This will trigger quiet a lot of cloud apps to outcast in the near future, similar to AWS CLI.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;With the rise in better internet services and bandwidth in the second and third world&amp;#39;s Dockerization/containerization will be emerging as a critical technology and on rise and will soon be a critical component in deployments.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Security - Cloud security should be a major concern for everyone working on cloud/IOT. One should perform a security assessment before starting their design. For IOT&amp;#39;s, using an RTOS does not ensure security and neither does Encryption. One should ensure all attack vectors are addressed. Even if you are able to secure the cloud, rest assured it may not be enough because your device can still be compromised.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 28 Jan 2016 15:48:00 +0530</pubDate>
        <link>http://hellodk.github.io///5_interesting_cloud_predictions_2016</link>
        <guid isPermaLink="true">http://hellodk.github.io///5_interesting_cloud_predictions_2016</guid>
        
        <category>tutorials</category>
        
        
        <category>dk</category>
        
      </item>
    
      <item>
        <title>Creating a Highly Available Distributed Messaging System</title>
        <description>&lt;p&gt;Creating a Highly Available Distributed Messaging Queue Cluster with RabbitMQ-3.5.6
What is rabbitmq?&lt;/p&gt;

&lt;p&gt;Rabbitmq is a robust yet easy to use messaging broker service for applications. Also referred as message-oriented middleware, rabbitmq implements Advanced Messaging Queue Protocol(AMQP).&lt;/p&gt;

&lt;p&gt;Rabbitmq server is written in Erlang programming language, and built on top of Open Telecom Platform framework for clustering and fail-over. Client libraries to interface with the broker are available for all major programming languages.&lt;/p&gt;

&lt;p&gt;Rabbitmq runs on all major operating systems and supports a huge number of developer platforms. Besides being open source, rabbitmq is also commercially supported.&lt;/p&gt;

&lt;p&gt;Firewall Settings(Listening ports):
amqp - 5672
clustering - 25672
management nodes - 15672&lt;/p&gt;

&lt;p&gt;How do we install rabbitmq?
Prerequisites:
Network connection between nodes must be reliable.&lt;/p&gt;

&lt;p&gt;All nodes must run the same version of Erlang and RabbitMQ.&lt;/p&gt;

&lt;p&gt;All TCP ports should be open between nodes, see the firewall section above.&lt;/p&gt;

&lt;p&gt;Host entries across the nodes should be the same:
In the /etc/hosts file of your system, append the hostname corresponding to the IP address, and the same data should be present across all the machines in the cluster to maintain uniformity. e.g. we have 3 machines:&lt;/p&gt;

&lt;p&gt;a.b.c.d with hostname as rabbitmq1
e.f.g.h with hostname as rabbitmq2
i.j.k.l with hostname as rabbitmq3
then in the /etc/hosts on all the 3 machines, add the below lines:&lt;/p&gt;

&lt;p&gt;a.b.c.d rabbitmq1
e.f.g.h rabbitmq2
i.j.k.l rabbitmq3
and that should suffice to solve your issues with host entries. Rabbitmq is very sensitive towards hostnames, hence this step is of quiet importance. Once done, do verify if the nodes are ping-able from every node.&lt;/p&gt;

&lt;p&gt;For installing rabbitmq-server, use the below commands:
sudo apt-get install -fy erlang-nox python-pip git-core python-setuptools git-core
wget https://www.rabbitmq.com/releases/rabbitmq-server/v3.5.6/rabbitmq-server&lt;em&gt;3.5.6-1&lt;/em&gt;all.deb
sudo dpkg -i rabbitmq-server&lt;em&gt;3.5.6-1&lt;/em&gt;all.deb
and there you go, your rabbitmq-server should be up and running. You can verifythis by the below command:&lt;/p&gt;

&lt;p&gt;sudo service rabbitmq-server status&lt;/p&gt;

&lt;p&gt;Now to setup your rabbitmq cluster with slave nodes, please follow the below steps:
sudo apt-get install -fy erlang-nox python-pip git-core python-setuptools git-core
sudo pip install pika==0.10.0 or pip install pika==0.10.0(if using virtual-env)
wget https://www.rabbitmq.com/releases/rabbitmq-server/v3.5.6/rabbitmq-server&lt;em&gt;3.5.6-1&lt;/em&gt;all.deb
sudo dpkg -i rabbitmq-server&lt;em&gt;3.5.6-1&lt;/em&gt;all.deb
Synchronizing the erlang.cookie file across all the machines:
This is again a very important step for setting up rabbitmq cluster. The file itself is storing the value without a carriage return nor a line feed. This value needs to go into the slaves the same way as it is on the master.&lt;/p&gt;

&lt;p&gt;First stop the rabbitmq-server on all the slave machines using the below command:&lt;/p&gt;

&lt;p&gt;sudo service rabbitmq-server stop
echo -n &amp;quot;&lt;contents of .erlang.cookie form master server&gt;&amp;quot; &amp;gt; /var/lib/rabbitmq/.erlang.cookie
eg: echo -n &amp;quot;DQRRLCTUGOBCRFNPIABC&amp;quot; &amp;gt; /var/lib/rabbitmq/.erlang.cookie&lt;/p&gt;

&lt;p&gt;The file should be exactly the same as it is on the master server also ensuring that the permission level for all these file is &amp;quot;400&amp;quot; and the ownership of this file remains with rabbitmq user.&lt;/p&gt;

&lt;p&gt;Rabbitmq uses mnesia db as it&amp;#39;s default, make sure you delete mnesia before starting rabbitmqserver:
sudo rm -rf /var/lib/mnesia&lt;/p&gt;

&lt;p&gt;Starting the rabbitmq-server:
Now that the .erlang.cookie file has been synced(manually) from the master node to all the slave nodes, and the mnesia db has also been deleted, we are good to go and start the rabbitmq-server.&lt;/p&gt;

&lt;p&gt;sudo service rabbitmq-server start
sudo rabbitmqctl stop&lt;em&gt;app
sudo rabbitmqctl reset
sudo rabbitmqctl join&lt;/em&gt;cluster rabbit@rabbitmq1 (assuming rabbitmq1 as master)
sudo rabbitmqctl start_app
RabbitMq can screw your cluster formation, if you do are not taking care of the host name and the entries inside /etc/hosts file properly.&lt;/p&gt;

&lt;p&gt;Check the cluster status using the below command:
sudo rabbitmqctl cluster_status&lt;/p&gt;

&lt;p&gt;Set the HA Policy:&lt;/p&gt;

&lt;p&gt;The following command will sync all the queues across all nodes:&lt;/p&gt;

&lt;p&gt;rabbitmqctl set_policy ha-all &amp;quot;&amp;quot; &amp;#39;{&amp;quot;ha-mode&amp;quot;:&amp;quot;all&amp;quot;,&amp;quot;ha-sync-mode&amp;quot;:&amp;quot;automatic&amp;quot;}&amp;#39;&lt;/p&gt;

&lt;p&gt;Enabling the user management plugin:
Rabbitmq also provides with a simple management console. The console can be enabled for each machine using the below command:&lt;/p&gt;

&lt;p&gt;sudo rabbitmq-plugins enable rabbitmq_management&lt;/p&gt;

&lt;p&gt;Now you can access the console on the following address: &amp;quot;http://&lt;rabbitmq machine ip with plugin enabled&gt;:15672&amp;quot;&lt;/p&gt;

&lt;p&gt;It will ask for user id and password. you need to provide the user id and password for authentication.&lt;/p&gt;

&lt;p&gt;In-case you have not yet created any user, the you can do so by the below commands:
sudo rabbitmqctl add&lt;em&gt;user &lt;user id&gt; &lt;password&gt;
sudo rabbitmqctl set&lt;/em&gt;user&lt;em&gt;tags &lt;user id&gt; administrator
sudo rabbitmqctl set&lt;/em&gt;permissions -p / &lt;user id&gt; &amp;quot;.&lt;em&gt;&amp;quot; &amp;quot;.&lt;/em&gt;&amp;quot; &amp;quot;.*&amp;quot;&lt;/p&gt;
</description>
        <pubDate>Sat, 19 Dec 2015 15:48:00 +0530</pubDate>
        <link>http://hellodk.github.io///Creating-a-Highly-Available-Distributed-Messaging-Queue-Cluster-with-RabbitMQ-3.5.6</link>
        <guid isPermaLink="true">http://hellodk.github.io///Creating-a-Highly-Available-Distributed-Messaging-Queue-Cluster-with-RabbitMQ-3.5.6</guid>
        
        <category>tutorials</category>
        
        
        <category>casper</category>
        
      </item>
    
      <item>
        <title>Understanding Git-Flow</title>
        <description>&lt;p&gt;Understanding Git Flow for(ubuntu-14.04):&lt;/p&gt;

&lt;p&gt;what is git flow?
Usage?
How can it be helpful?&lt;/p&gt;

&lt;p&gt;Install git flow using the below command:&lt;/p&gt;

&lt;p&gt;sudo apt-get install git-flow&lt;/p&gt;

&lt;p&gt;Starting with git flow:
To start with git flow, you first need to initialize it inside an existing repository. This is done by using the below command:&lt;/p&gt;

&lt;p&gt;git flow init&lt;/p&gt;

&lt;p&gt;In-case you want to force re-initialization of git flow, you can use the same command and pass -f as an argument
eg: git flow init -f&lt;/p&gt;

&lt;p&gt;So far cool, this is dead simple, isn&amp;#39;t it? Now let&amp;#39;s see what happens once we do a git flow init. We&amp;#39;ll have to answer a few questions regarding the naming conventions for our branches. For simplicity it is recommended to use the default values.&lt;/p&gt;

&lt;p&gt;$ git flow init -f&lt;/p&gt;

&lt;p&gt;Which branch should be used for bringing forth production releases?
   - develop
   - feature/lint_cleanup
   - master
Branch name for production releases: [master] &lt;/p&gt;

&lt;p&gt;Which branch should be used for integration of the &amp;quot;next release&amp;quot;?
   - develop
   - feature/lint_cleanup
Branch name for &amp;quot;next release&amp;quot; development: [develop] &lt;/p&gt;

&lt;p&gt;How to name your supporting branch prefixes?
Feature branches? [feature/] 
Release branches? [release/] 
Hotfix branches? [hotfix/] 
Support branches? [support/] 
Version tag prefix? [] &lt;/p&gt;

&lt;p&gt;Creating/Deleting a tag:&lt;/p&gt;

&lt;p&gt;Deleting a tag:&lt;/p&gt;

&lt;p&gt;$ git tag -d &lt;tag_name&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 09 Nov 2015 00:00:00 +0530</pubDate>
        <link>http://hellodk.github.io///understanding_git_flow</link>
        <guid isPermaLink="true">http://hellodk.github.io///understanding_git_flow</guid>
        
        <category>tutorials</category>
        
        
        <category>dk</category>
        
      </item>
    
      <item>
        <title>Going Agntless with Ansible</title>
        <description>&lt;p&gt;While looking for a light yet powerful Configuration Management tool, I was going through some of the various available tools, and came across Ansible. After having a quick look at the features, something which caught my attention: &amp;quot;Ansible is agent-less&amp;quot;, hence I decided to give it a try. Having used Ansible for some around more than 8+ months configuration management, I personally feel...Ansible is awesome!!
So what is Ansible? Well, it is a configuration management tool, and it empowers you to configure/orchestrate multiple systems at great ease, saving a lot of downtime and resources.
So how does it does that? The answer lies with ssh keys based agent forwarding. Ansible uses ssh for logging to the remote system, and can execute package installation, shell commands, install/update package management systems, clone git and anything under the sun you need for configuration management.
So why don&amp;#39;t you try your hands on Ansible and experience it&amp;#39;s awesomeness....all you need to do is to install it on your host system.&lt;/p&gt;

&lt;p&gt;Ansible does not have it&amp;#39;s own process, and unlike other configuration tolls available, it is agent-less!!&lt;/p&gt;

&lt;p&gt;Ansible ships in 2 forms:
1. Core Ansible
2. Ansible Tower&lt;/p&gt;

&lt;p&gt;Installation:
1. OS package manager:
sudo apt-get install software-properties-common
sudo apt-add-repository ppa:ansible/ansiblesudo apt-get update
sudo apt-get install ansible
2. Via python pip:
sudo pip install ansible&lt;/p&gt;

&lt;p&gt;Ansible uses a configuration file, called host files for doing ssh to the remote machines. By default it is located in /etc/ansible/hosts
All you need to do is to add the host ip and the ssh_username to the file, and the public key of your host to the remote machine and you are all set to go!!&lt;/p&gt;

&lt;p&gt;Commands:
ansible all -i ansible_hosts -m ping
ansible all -m ping -u deepak
ansible -m shell -a &amp;#39;free -m&amp;#39; host1
ansible all -m shell -a &amp;#39;free -m&amp;#39; -u dk&lt;/p&gt;

&lt;p&gt;Playbooks are Ansible’s configuration, deployment, and orchestration language. They can describe a policy you want your remote systems to enforce, or a set of steps in a general IT process.&lt;/p&gt;

&lt;p&gt;Playbooks are written in YAML language, and comprises of Plays&lt;/p&gt;

&lt;p&gt;Running a Playbook in Ansible:
ansible-playbook playbook.yml -f 10&lt;/p&gt;
</description>
        <pubDate>Mon, 06 Apr 2015 15:48:00 +0530</pubDate>
        <link>http://hellodk.github.io///going_agentless_with_ansible</link>
        <guid isPermaLink="true">http://hellodk.github.io///going_agentless_with_ansible</guid>
        
        <category>tutorials</category>
        
        
        <category>dk</category>
        
      </item>
    
      <item>
        <title>Camera Basics</title>
        <description>&lt;p&gt;A basic tutorial on handling a DSLR for beginners.&lt;/p&gt;
</description>
        <pubDate>Wed, 28 Aug 1963 15:48:00 +0530</pubDate>
        <link>http://hellodk.github.io///camera-basics</link>
        <guid isPermaLink="true">http://hellodk.github.io///camera-basics</guid>
        
        <category>tutorials</category>
        
        <category>photography</category>
        
        
        <category>casper</category>
        
      </item>
    
  </channel>
</rss>
