<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
   
      <title>https://hellodk.in/</title>
   
   <link>http://hellodk.in/</link>
   <description>A beautiful narrative written over an elegant publishing platform. The story begins here...</description>
   <language>en-uk</language>
   <managingEditor> Deepak Gupta</managingEditor>
   <atom:link href="rss" rel="self" type="application/rss+xml" />
   
	<item>
	  <title>My Resume Hosted</title>
	  <link>//resume</link>
	  <author>Deepak Gupta</author>
	  <pubDate>2020-04-02T00:00:00+05:30</pubDate>
	  <guid>//resume</guid>
	  <description><![CDATA[
	     <!DOCTYPE html>
<html>

<style>
.tab-1 {position:absolute;left:180px; }

.tab-2 {position:absolute;left:300px; }

p.small {
  line-height: 0.7;
}

p.big {
  line-height: 1.8;
}
</style>

<head>
	<link rel="stylesheet" href="/assets/css/hr_tag.css" />
	<title>Curriculum Vitae</title>
</head>

<body>
<h1>Deepak Gupta</h1>
<ul>
<li>Email: <span class="tab-1">hello.dk@outlook.com</li>
<li>About.me: <span class="tab-1"><a href="https://about.me/hellodk">about.me/hellodk</a></li>
<li>Blog: <span class="tab-1"><a href="https://www.hellodk.in">hellodk.io</a></li></li>
<li>Github: <span class="tab-1"><a href="https://github.com/hellodk">github.com/hellodk</a></li></li>
<li>LinkedIn: <span class="tab-1"><a href="https://www.linkedin.com/in/hellodk">linkedin.com/in/hellodk</a></li></li>
<li>Docker Hub: <span class="tab-1"><a href="https://hub.docker.com/r/hellodk">hub.docker.com/r/hellodk</a></li></li>
</ul>

<hr class="bigHr">

<h2>Work Experience:</h2>

<ul>
<li><h3>DevOps Consultant/Trainer, Bengaluru</h3></li>
<h5>June 2018 - PRESENT</h5>
<p>I've helped organisations adopt DevOps tooling/practices, scaling strategies & have delivered 200+ corporate trainings on DevOps tools across the globe to a broad set of audiences(Developers/Sysadmins, Freshers, Architects, CTO's, VP) with nearabout 95% success rate.</p>

<li><h3>DevOps Lead - Moveinsync Technology, Bengaluru</h3></li>
<h5>January 2018 - June 2018</h5>
<p>Moveinsync is India's chief employee transportation management solution. I had lead the DevOps team at Moveinsync to build & monitor a highly scalable multi-tenant application on cloud.
I've contributed to VAPT, Implementing DR and achieve government certification which helped us to secure more clients.</p>

<li><h3>Systems Engineer - Myntra Designs, Bengaluru</h3></li>
<h5>June 2016 - January 2018</h5>
<p>Myntra Designs is the biggest Indian fashion e-commerce organisation in India.
I was a part of the sysadmin team managing 99.9999% uptime of infrastructure(Datacenter, AWS & Azure), monitoring microservices, API calls, revenue metrics, on-calls, writing internal tools, automating deployments,  database maintenance etc.
</p>

<li><h3>DevOps Engineer - Knowlarity Communications, Bengaluru</h3></li>
<h5>January 2015 - May 2016</h5>
<p>Knowlarity Communications works on AI enabled cloud telephony.
My primary responsibilities were to automate deployments, maintain databases and write API's to scrub data upto 440 million records, take care of the Billing framework model, creating grafana dashboards, backup and recovery, maintain rabbitmq clusters.
</p>

<li><h3>Project Engineer - Wipro Technologies, Bengaluru</h3></li>
<h5>November 2011 - January 2015</h5>
<p>Wipro Technologies is an Indian MNC providing IT consulting & services.
I was primarily responsible to write applications in Java for telecom OSS and also creating API's for Openstack implementation.</p>
</br>
</ul>

<hr class="bigHr">
<h2>Software Skills:</h2>
<ul>
<!---li>Programming: 	Python, Java, Golang, Nodejs, C(Agile/Kanban Methodology)</li>
<li>Web Frameworks: Django, Spring Boot, Flask, Falcon, Spring Cloud</li>
<li>RDBMS:			MySQL, Postgresql, MariaDB</li>
<li>NoSQL: Cassandra, Dynamodb, Rethinkdb, MongoDB, Redis, CouchDB</li>
<li>Monitoring Tools: Nagios, Shinken, Zabbix, Sensu, Icinga2</li>
<li>Build Tools: Jenkins, Jira, Gerrit</li>
<li>Cryptocurrency: Blockchain, Bitcoin, Ethereum, Hyperledger</li>
<li>Load balancers: HA Proxy, Nginx</li>
<li>CDN: Akamai, CloudFront, Cloudflare</li>
<li>Web/App servers: Nginx, Apache, Gunicorn, uwsgi, tomcat</li>
<li>Configuration Management: Ansible, Saltstack, Fabric, Puppet, Chef</li>
<li>Protocols/ Architectures:	REST, CORBA, SNMP, HTTP, TCP/IP, SIP, Wireshark</li>
<li>Cloud/ Virtualizations: AWS, Azure, Heroku, OpenStack, Vagrant, KVM, Docker</li>
<li>Visualizations: Grafana, D3, Kibana, Talend</li>
<li>Others: RaspberryPi, Spartan 3E, AVR, Elasticsearch, Induino, Arduino, MOSHELL, Debian Packaging, freeswitch, WCDMA, LTE, 3PP, NMS, EMS, FCAPS, RNC, RBS, Scribe, Logstash, Fluentd,heka</li--->
<li>Cloud Computing: <span class="tab-2">  AWS, Azure, Heroku, Openstack</li>
<li>Container Technologies: <span class="tab-2">  Docker, Kubernetes</li>
<li>Monitoring Tools: <span class="tab-2">  Zabbix, Nagios, Icinga2</li>
<li>SQL Databases: <span class="tab-2">  MySQL, PostgreSQL, MariaDB</li>
<li>NoSQL Databases: <span class="tab-2">  MongoDB, Cassandra, Redis</li>
<li>Web Server/Load Balancers: <span class="tab-2">  Nginx, HA Proxy</li>
<li>Messaging Tools: <span class="tab-2">  RabbitMQ, Kafka</li>
<li>Configuration Management: <span class="tab-2">  Ansible, Terraform, Chef, Puppet</li>
<li>Programming: <span class="tab-2">  Java, Python, Golang</span></li>
<li>Visualizations: <span class="tab-2">  Grafana, D3, Kibana, Talend</li>
</ul>

<hr class="bigHr">
<h2>
	Projects Summary:
</h2>
<ul>
	<li>
<h3>Disaster Recovery
</h3></li>
<p>Creating DR infrastructure, requirement gathering and creation of Kubernetes cluster on bare metal servers and implementing the deployment pipelines - blue-green and canary
Infrastructure & service monitoring, sending alerts over slack and SMS
</p>

<li>
	<h3>Payments Service:
</h3>
</li>
<p>
Create payments service for facilitating payments transactions using Java and Spring Boot and implementing analytics with Talend to monitor the payments/orders. Invoved in Sprint Planning, Requirement gathering, Architecture planning, writing unit test-cases, coding configuration of the cluster, managing shards/replicas of the payments database, coordinating UAT and SIT and load tests
</p>

<li>
	<h3>Centralized Log Management:
</h3>
</li>
<p>
To monitor logs centrally, we needed a powerful tool. Elasticsearch is what we choose for this project and developed on top of Java using Spring Cloud. Initiated the requirement gathering, created UML diagrams, architecture planning, automated deployment & configuration of the cluster, managing shards and the replicas for elasticsearch cluster, analytics on the data using talend
</p>

<li>
	<h3>
PCI Compliance:
</h3>
</li>
<p>
Ensured the Payments setup is PCI DSS compliant by creating network segmentations for servers(DMZ environment) and implementing Intrusion Detection Systems(OSSEC/Alienvault) & patching(Spacewalk) the air gapped systems. Responsible for getting the VAPT(Vulnerability Assessment & Penetration Testing)
</p>

<li>
	<h3>
	Apollo:
</h3>
</li>
<p>
App deployment via one click using Ansible, Docker and Kubernetes by  automatically creating templates for tasks using jinja2 templating systems and wrote executors, setting up Jenkins jobs etc.
</p>

<li>
	<h3>
Sethji:
</h3>
</li>
<p>
Track AWS/Azure Billing Charges
Bill analysis using ETL & Setup the billing management stack on python and flask
reduced billing costs by 25% by identifying overprovisioned/unused services etc.
</p>

<li>
	<h3>
	Graphite Grafana Integration
</h3>
</li>
<p>
monitor services, function calls, throughput, response code status, revenue etc.
Requirement gathering, UML, coding, writing automation, configuration and deployment on Python
</p>

<li>
<h3>
	Monitoring Setup
</h3>
</li>
<p>
	Monitoring for complete Infrastructure
Setup monitoring for our infrastructure(hybrid) over Icinga2/Zabbix and Talend
Ensured High Availability of Services
</p>

<li><h3>Daily Operations - A usual day in the life of a DevOps</h3></li>
<p>
Developing tools over C++/Java/Python/Golang</br>
Security Audit - Implemented IDS, DDOS mitigation via fail2ban</br>
Packet tracing/filtering using customized tool & Wireshark</br>
Fixing security vulnerabilities in infrastructure</br>
Implemented key rotation policy</br>
Implemented HA and reliable RabbitMQ cluster in the infrastructure which served as a backbone for intercommunication between microservices(close to 400+ microservices) with 99.99999% SLA</br>
DNS, LDAP, Monitoring, Load Balancing over Nginx/HA Proxy</br>
Reduce data transfer costs & improved performance</br>
Subnet planning helped reduce the complexity of whitelisting services/IP’s</br>
Network planning for infrastructure migration</br>
Helped setup Azure account with basic services like DNS, LDAP, monitoring</br>
</p>

<li>
	<h3>
	Clickstream Analytics
</h3>
</li>
<p>
A single point to handle all clickstream data and do analytics on that
Integrating the existing SQL databases with ETL(Talend) and creating dashboards
Fine details like demography, geographic locations, time etc were extracted
Used the analytics data to create recommendation engine
</p>

<li>
<h3>
	WRAN CM OSS-RC (Operational Support System – Radio & Core)
</h3>
</li>
<p>
OSS-RC is a comprehensive domain manager for network infrastructure deployed with operators around the world integrating and managing a wide range of network components. Together with IP and Broadband offering, it’s a comprehensive solution for total network management of the telecommunications infrastructure
Design of OSSRC products, configuring network elements of OSS-RC using Spring/Java
Sprint Planning, Requirement gathering, Implementation for new changes proposed, creating user stories
followed Test Driven Development, Coordinating in SIT, UAT
</p>
<li>
<h3>
	Cloud Adapter
</h3>
</li>
<p>
Integrate cloud services with services on physical machines for centralized monitoring
Set up the development environment, configurations, writing test cases using J-unit
wrote authentication modules, schedulers, startup scripts, managing notifications on Java
</p>
<li>
<h3>
	Billing Framework
</h3>
</li>
<p>
The most challenging work for any organisation, taking care of different types of contracts etc.
Created the Billing Framework using Python and Django
</p>
<li>
<h3>
	NDNC Deployment
</h3>
</li>
<p>
Being a telemarketer, we can call only to non-dnd registered numbers
TRAI only provides dnd data in form of CSV(500 million rows)
Challenge was to develop our own NDNC scrubbing solution and keep it updated using Spring Boot
Requirement gathering, UML/flow diagrams getting the NDNC data, feeding the data into our database, writing API's, automated product deployment automation, performance tuning etc. using Python/Falcon and RethinkDB
</p></ul>
<hr class="bigHr">
<h3>
	Side Projects
</h3>
<ul>
<p>
<li>Developed several multiplayer games in Python, e.g. Stopwatch, Pong, Memory, Spaceship, Blackjack & Rice Rocks Full Game
</li>
</p>

<p>
<li>
RTL design & Synthesis of a 32-bit Microprocessor using VHDL
</li>
Our goal was to design a 32-bit microprocessor in VHDL, which will perform arithmetic and logic function that is on a standard 32-bit microprocessor
Target Device: Spartan 3E Tools
Used: Xilinx 9.1, Modelsim SE 5.7f
</p>

<p>
<li>
Blockchain Signalling System
</li>
Used blockchain for Signalling DDOS attacks in a cooperative & distributed network defence
</p>

<p>
<li>
Real-time Bitcoin Price Monitor using Arduino
</li>
</p>

<p>
	<li>
Decentralized fleet tracking with blockchain
</li>
Asset tracking mechanism in a decentralized fashion. Each action, event, alerts were stored in the blockchain
</p>

<p>
<li>
Developed an own cryptocurrency for testing purpose using Litecoin
</li>
</p>
</ul>

<hr class="bigHr">

<h3>
	Awards & Achievements:
</h3>

<ul>
<li>Maestro Award for Making a Difference in the Account</li>
<li>High Flyer Award for individual contribution in the Account</li>
</ul>

<hr class="bigHr">

<h3>
Certifications:
</h3>

<ul>
	<li>Big Data, Cloud Computing, & CDN Emerging Technologies</li>
	<li>Blockchain for Developers</li>
	<li>Interfacing with the Raspberry Pi</li>
	<li>An Introduction to Interactive Programming with Python(RICE University)</li>
</ul>

</body>
</html>
	  ]]></description>
	</item>

	<item>
	  <title>REST API Best Practices</title>
	  <link>//travel_diaries</link>
	  <author>Deepak Gupta</author>
	  <pubDate>2017-12-14T04:18:00+00:00</pubDate>
	  <guid>//travel_diaries</guid>
	  <description><![CDATA[
	     <html>
REST API ARCHITECTURE – BEST PRACTICES

SOA(Service Oriented Architecture) has become a foundation for the most of the application that are developed today</br>
A service oriented architecture is an architectural pattern which enables collection of services to communicate with external/internal parties to pass data or for services coordinating</br>
Let’s start from what’s a service.
What’s a Service/API
A service is a function that is well-defined, self-contained, and does not depend on the context or state of other services.
In other words an interface used by software components to communicate with each other</br>
Let understand the difference between API and Web Services</br>
A Web Service is a type of API, almost always one that operates over HTTP</br>
In the modern world there are two types of web services that are used.
SOAP
REST
We can’t compare REST and SOAP since SOAP is a protocol and REST is an architectural pattern</br>
People mostly get confused when selecting one for their application</br>
If you are wondering what’s the difference between these two you can get a good start from here and here.
Our today’s focus is on REST</br>
How we can implement a perfect REST API? What are the best practices? Let’s get down to business.
REST Best Practices 101
1</br>
Abstract vs Concrete
When designing a REST API you should consider to make API concrete as possible</br>
It will make the API less confusing to the consumers.
Abstract vs Concrete 
Abstract vs Concrete
2</br>
CRUD Operations
There are four available methods when designing a REST API which are GET, POST, PUT and DELETE</br>
Below is the proposed methodology to implement CRUD operations in a REST API</br>
Note that this is suggested by me and you can alter this as per your requirement.
Resource	POST	GET	PUT	DELETE
/dog	Create a new dog	List dogs	Replace dogs with new dogs(Bulk update)	 Delete all dogs
/dog/1234	Error	 Show dog	If exist update dog else ERROR	 Delete dog
 
I know there is a confusion resolve around PUT and DELETE</br>
Read more here and here to clarify.
3</br>
Error Handling
Error handling is one that get less attention but most important part of the any REST API</br>
You must give hints as possible for the API consumers about the error and why it has occurred</br>
Also make you that through the API you should provide granular level error messages</br>
You can format it as follows.

{
   "status": 401,
   "error_code": 2005,
   "error_message": "Authentication token has expired",
   "more_info": "http://dasunhegoda.com/api/doc/token_error"
}
1
2
3
4
5
6
{
   "status": 401,
   "error_code": 2005,
   "error_message": "Authentication token has expired",
   "more_info": "http://dasunhegoda.com/api/doc/token_error"
}
You can make use of HTTP status code for this purpose.
4</br>
API Versioning
In any given API API version is mandatory to maintain consistency</br>
It can be done in many way but below is the preferred methods by me.
Method 1
You can use the letter ‘v’ in the URL to denote the API version as below.

http://dasunhegoda.com/api/v2/dog/1234
1
http://dasunhegoda.com/api/v2/dog/1234
Method 2
You can use the addional parameter at the end of the URL.

http://dasunhegoda.com/api/dog/1234?v=2.0
1
http://dasunhegoda.com/api/dog/1234?v=2.0
Different people have different opinion on API version</br>
You can read more here.
5</br>
Filtering
Don’t provide unnecessary data to the API consumers</br>
It will clutter you REST API unnecessarily</br>
Let the developer choose what he needs</br>
For this we can use filtering methods in our APIs.

/dogs/1234?value1,value2,value3,value4
1
/dogs/1234?value1,value2,value3,value4
Also you can use pagination for this purpose as well where you don’t have to return all the results at once</br>
Below query will be familiar to you since it operates same as MySQL works.

/dogs?limit=25&offset=20

You can read more <a href="http://stackoverflow.com/questions/5020704/how-to-design-restful-search-filtering" target="_blank">here</a>.
1
2
3
/dogs?limit=25&offset=20
 
You can read more <a href="http://stackoverflow.com/questions/5020704/how-to-design-restful-search-filtering" target="_blank">here</a>.
6</br>
Security
Security is one of the major concerns when compared to SOAP because still there are no standards such as ws-security defined for REST.
You can use HTTPS across your APIs.
Don’t forget to include timestamp in each and every API request and response</br>
Make sure to log them all</br>
In case of a dispute you can refer them.
Use a access_token to make sure that API is invoked by the trust parties</br>
Beforehand you have to deliver the access_token whereas only API consumers have an access_token can invoke the API</br>
Read more.
7</br>
Analytics
Once you start logging each and every API request and response you can build a analytical platform on top of that</br>
If the number of records are high you might have to consider technologies such as BigData</br>
Having analytics in your REST API will give you a good insight of what’s happening your API.
8</br>
Documentation
Proper Documentation is vital for the API</br>
It doesn’t matter how great your API design is if the API consumers can’t used it properly</br>
You can use tools such as apidocjs for this purpose</br>
It’s really easy to get started.
9</br>
Stability and Consistency
Depending on your requirement you should consider highly available architecture for you REST API</br>
If you are wondering how to implement high availability in your REST API I have an article written on the subject</br>
Please refer here.
10</br>
URL Structure
You have to structure the URL in manner it’s intuitive</br>
Select a domain which is easy for marking as well</br>
eg :- api.yourdomain.com</br>
When structuring your REST API you can use the following format.
GET tasks/5/messages – Retrieves list of messages for task #5
GET tasks/5/messages/10 – Retrieves the 10th messages for task #5
POST tasks/5/messages – Create a new message for task #5
DELETE tasks/5/messages/10 – Delete the 10th messages of task #5
PUT tasks/5/messages/12 – Update the 12th messages of task #5
If you have developed your REST API properly you should have above features in it</br>
In other words above should be kept in mind when designing your REST API</br>
So that’s it about REST API architecture</br>
If you have any questions let me know in the comments below</br>
Your feedback is highly appreciated(happy-face)
</html>
	  ]]></description>
	</item>

	<item>
	  <title>Python regex Simplified</title>
	  <link>//python_regex_simplified</link>
	  <author>Deepak Gupta</author>
	  <pubDate>2017-07-13T03:30:00+00:00</pubDate>
	  <guid>//python_regex_simplified</guid>
	  <description><![CDATA[
	     <h2><u>Introduction</u></h2>

<h3>Python regex metacharacters</h3>
. ^ $ * + ? { } [ ] \ | ( )

<p>

1. [] - 

Negate - ^

Escape Character - \


The first metacharacters we’ll look at are [ and ]. They’re used for specifying a character class, which is a set of characters that you wish to match. Characters can be listed individually, or a range of characters can be indicated by giving two characters and separating them by a '-'. For example, [abc] will match any of the characters a, b, or c; this is the same as [a-c], which uses a range to express the same set of characters. If you wanted to match only lowercase letters, your RE would be [a-z].

Metacharacters are not active inside classes. For example, [akm$] will match any of the characters 'a', 'k', 'm', or '$'; '$' is usually a metacharacter, but inside a character class it’s stripped of its special nature.

You can match the characters not listed within the class by complementing the set. This is indicated by including a '^' as the first character of the class; '^' outside a character class will simply match the '^' character. For example, [^5] will match any character except '5'.

Perhaps the most important metacharacter is the backslash, \. As in Python string literals, the backslash can be followed by various characters to signal various special sequences. It’s also used to escape all the metacharacters so you can still match them in patterns; for example, if you need to match a [ or \, you can precede them with a backslash to remove their special meaning: \[ or \\.

Some of the special sequences beginning with '\' represent predefined sets of characters that are often useful, such as the set of digits, the set of letters, or the set of anything that isn’t whitespace.

Let’s take an example: \w matches any alphanumeric character. If the regex pattern is expressed in bytes, this is equivalent to the class [a-zA-Z0-9_]. If the regex pattern is a string, \w will match all the characters marked as letters in the Unicode database provided by the unicodedata module. You can use the more restricted definition of \w in a string pattern by supplying the re.ASCII flag when compiling the regular expression.

The following list of special sequences isn’t complete. For a complete list of sequences and expanded class definitions for Unicode string patterns, see the last part of Regular Expression Syntax in the Standard Library reference. In general, the Unicode versions match any character that’s in the appropriate category in the Unicode database.

\d
Matches any decimal digit; this is equivalent to the class [0-9].
\D
Matches any non-digit character; this is equivalent to the class [^0-9].
\s
Matches any whitespace character; this is equivalent to the class [ \t\n\r\f\v].
\S
Matches any non-whitespace character; this is equivalent to the class [^ \t\n\r\f\v].
\w
Matches any alphanumeric character; this is equivalent to the class [a-zA-Z0-9_].
\W
Matches any non-alphanumeric character; this is equivalent to the class [^a-zA-Z0-9_].
These sequences can be included inside a character class. For example, [\s,.] is a character class that will match any whitespace character, or ',' or '.'.

The final metacharacter in this section is .. It matches anything except a newline character, and there’s an alternate mode (re.DOTALL) where it will match even a newline. '.' is often used where you want to match “any character”.
</p>
	  ]]></description>
	</item>

	<item>
	  <title>Mac Shortcuts</title>
	  <link>//mac_shortcuts</link>
	  <author>Deepak Gupta</author>
	  <pubDate>2017-07-05T10:18:00+00:00</pubDate>
	  <guid>//mac_shortcuts</guid>
	  <description><![CDATA[
	     <h3>Keyboard Tips</h3>
<ol>
<li>The Delete key works like Backspace if you are from other platforms. To delete on the other direction, press Fn + Delete</li>
<li>The Backspace will feel slow by default. This can be changed in Keyboard Preferences by making "Key Repeat" fast & "Delay Until Repeat" short. Ref: http://forums.macrumors.com/showthread.php?t=508385 </li>
<li>If you are missing Home & End keys, it is Command+ Left Arrow & Command + Right Arrow</li>
<li>If you are missing Page Up & Down keys, it is Command+ Up Arrow & Command + Down Arrow</li>
 
<h3>Mouse/Trackpad Tips:</h3>
To Drag something, Select Text, you can press the mouse button and drag it. If you are familiar with Double Tap & Drag like in Windows, Linux, You can have the same settings enabled in System Preferences > Universal Access > Mouse & Trackpad >Trackpad Options -> Enable Dragging Without Lock. Ref: http://chris.dziemborowicz.com/blog/2012/07/04/enable-double-tap-to-drag-in-mac-os-x-lion/
 
Navigation:
Command+Down To Open a Folder/File or to launch an application from keyboard
Command + Up   To go one level up in finder
Ctrl + Up             To Bring Mission Control
Command + tilde  To Switch windows of same App
Command + h       To hide the window
/                          To enter/paste a path in Finder Window (Open, Save Dialog boxes)
 
Terminal:
Ctrl+a  To go to the beginning of line, Ctrl+e to go the end of line. The Link has more shortcuts: http://apple.stackexchange.com/questions/12997/can-home-and-end-keys-be-mapped-when-using-terminal
Tweaks:
Show Full Path in the title bar:
defaults write com.apple.finder _FXShowPosixPathInTitle -bool YES
killall Finder
 Ref: http://osxdaily.com/2007/12/02/show-full-directory-path-in-finder-window-title-bars/
Copy Path of selected Folder/File:
Ref: http://osxdaily.com/2013/06/19/copy-file-folder-path-mac-os-x/
	  ]]></description>
	</item>

	<item>
	  <title>Yum Commands - A quick jist</title>
	  <link>//yum_commands_quick_reference</link>
	  <author>Deepak Gupta</author>
	  <pubDate>2017-06-10T12:20:00+00:00</pubDate>
	  <guid>//yum_commands_quick_reference</guid>
	  <description><![CDATA[
	     <ol>
<li><h5>To make a search of some package or term in the data base in some of the formed deposits yum in the system:</h5></li>
</br>
<p>Syntax: <span style="font-family: courier new,courier;">yum search any-package</span></p>
Example: <code>yum search httpd</code>
<h5>To consult the information contained in a package in individual:</h5>
</br>yum info any-package
</br>Example: yum info httpd
</br></br>Uninstalling packages. Desinstalación of packages along with everything what it depends on these:
</br>yum remove any-package
</br>Example: yum remove gkrellm
</br>The following thing will list all the packages available in the data base yum and that can settle:
</br>available yum list|less
</br>The following thing will list all the packages installed in the system:
</br>yum list installed|less
</br>The following thing will list all the packages installed in the system and that can (they must) be updated:
</br>yum list updates|less
</br>Cleaning of the system.
</br>
<p><span style="font-family: courier new,courier;">Yum leaves as result of its use heads and packages RPM stored in the interior of the directory located in the route /var/cache/yum/. Particularly the packages RPM that have settled can occupy much space and is by such reason agrees to eliminate them once no longer they have utility. Also it agrees to do the same with the old heads of packages that no longer are in the data base. In order to make the </br>corresponding cleaning, the following thing can be executed:</span></p>s
</br>
</br><code>yum clean all</code>
</br>Group install:
</br><code>yum groupinstall "groupname"</code>
</ol>
	  ]]></description>
	</item>

	<item>
	  <title>Configure multiple SSH identities for GitBash, Mac OSX, & Linux</title>
	  <link>//configure_multiple_ssh_identities_for_gitbash_mac_linux</link>
	  <author>Deepak Gupta</author>
	  <pubDate>2017-04-26T13:18:00+00:00</pubDate>
	  <guid>//configure_multiple_ssh_identities_for_gitbash_mac_linux</guid>
	  <description><![CDATA[
	     <p>
</p>
	  ]]></description>
	</item>

	<item>
	  <title>RabbitMQ - Detecting Dead TCP Connections with Heartbeats</title>
	  <link>//detecting-dead-tcp-connections</link>
	  <author>Deepak Gupta</author>
	  <pubDate>2017-04-24T12:18:00+00:00</pubDate>
	  <guid>//detecting-dead-tcp-connections</guid>
	  <description><![CDATA[
	     <p>
In some types of network failure, packet loss can mean that disrupted TCP connections take a moderately long time (about 11 minutes with default configuration on Linux, for example) to be detected by the operating system. AMQP 0-9-1 offers a heartbeat feature to ensure that the application layer promptly finds out about disrupted connections (and also completely unresponsive peers). Heartbeats also defend against certain network equipment which may terminate "idle" TCP connections. See Heartbeats for details.
</p>

<h4><u>At the Broker</u></h4>
<p>
In order to avoid losing messages in the broker we need to cope with broker restarts, broker hardware failure and in extremis even broker crashes.
</p>
<p>
To ensure that messages and broker definitions survive restarts, we need to ensure that they are on disk. The AMQP standard has a concept of durability for exchanges, queues and of persistent messages, requiring that a durable object or persistent message will survive a restart. More details about specific flags pertaining to durability and persistence can be found in the AMQP Concepts Guide.
</p>
<h4><u>Clustering and High Availability</u></h4>
<p>
If we need to ensure that our broker survives hardware failure, we can use RabbitMQ's clustering. In a RabbitMQ cluster, all definitions (of exchanges, bindings, users, etc) are mirrored across the entire cluster. Queues behave differently, by default residing only on a single node, but optionally being mirrored across several or all nodes. Queues remain visible and reachable from all nodes regardless of where they are located.
</p>
<p>
Mirrored queues replicate their contents across all configured cluster nodes, tolerating node failures seamlessly and without message loss (although see this note on unsynchronised slaves). However, consuming applications need to be aware that when queues fail their consumers will be cancelled and they will need to reconsume - see the documentation for more details.
</p>
At the Producer
<p>
When using confirms, producers recovering from a channel or connection failure should retransmit any messages for which an acknowledgement has not been received from the broker. There is a possibility of message duplication here, because the broker might have sent a confirmation that never reached the producer (due to network failures, etc). Therefore consumer applications will need to perform deduplication or handle incoming messages in an idempotent manner.
</p>
<h4><u>Ensuring Messages are Routed</u></h4>
<p>
In some circumstances it can be important for producers to ensure that their messages are being routed to queues (although not always - in the case of a pub-sub system producers will just publish and if no consumers are interested it is correct for messages to be dropped).
</p>
<p>
To ensure messages are routed to a single known queue, the producer can just declare a destination queue and publish directly to it. If messages may be routed in more complex ways but the producer still needs to know if they reached at least one queue, it can set the mandatory flag on a basic.publish, ensuring that a basic.return (containing a reply code and some textual explanation) will be sent back to the client if no queues were appropriately bound.
</p>
<p>
Producers should also be aware that when publishing to a clustered node, if one or more destination queues that are bound to the exchange have mirrors in the cluster, it's possible to incur delays in the face of network failures between nodes, due to flow control between replicas and the master queue process. See here for more details.
</p>
<h4><u>At the Consumer</u></h4>
<p>
In the event of network failure (or a node crashing), messages can be duplicated, and consumers must be prepared to handle them. If possible, the simplest way to handle this is to ensure that your consumers handle messages in an idempotent way rather than explicitly deal with deduplication.
</p>
<p>
If a message is delivered to a consumer and then requeued (because it was not acknowledged before the consumer connection dropped, for example) then RabbitMQ will set the redelivered flag on it when it is delivered again (whether to the same consumer or a different one). This is a hint that a consumer may have seen this message before (although that's not guaranteed, the message may have made it out of the broker but not into a consumer before the connection dropped). Conversely if the redelivered flag is not set then it is guaranteed that the message has not been seen before. Therefore if a consumer finds it more expensive to deduplicate messages or process them in an idempotent manner, it can do this only for messages with the redelivered flag set.
</p>
<h4><u>Consumer Cancel Notification</u></h4>
<p>
Under some circumstances the server needs to be able to cancel a consumer - since the queue it was consuming from has been deleted, or has failed over. In this case the consumer should consume again but be aware that it may see messages again which it has already seen.
</p>
<code>Note that consumer cancel notification is a RabbitMQ extension to AMQP, and as such may not be supported by all clients.</code>
</br></br>
<h4><u>Messages That Cannot Be Processed</u></h4>
<p>
If a consumer determines that it cannot handle a message then it can reject it using basic.reject (or basic.nack), either asking the server to requeue it, or not (in which case the server might be configured to dead-letter it instead.
</p>
<h4><u>Distributed RabbitMQ</u></h4>
<p>
Rabbit provides two plugins to assist with distributing nodes over unreliable networks: federation and the shovel. Both are implemented as AMQP clients, so if you configure them to use confirms and acknowledgements, they will retransmit when necessary. Both will use confirms and acknowledgements by default.
</p>
<p>
When connecting clusters with federation or the shovel, it is desirable to ensure that the federation links and shovels tolerate node failures. Federation will automatically distribute links across the downstream cluster and fail them over on failure of a downstream node. In order to connect to a new upstream when an upstream node fails you can specify multiple redundant URIs for an upstream, or connect via a TCP load balancer.
</p>
<p>
When using the shovel, it is possible to specify redundant brokers in a source or destination clause; however it is not currently possible to make the shovel itself redundant. We hope to improve this situation in the future; in the mean time a new node can be brought up manually to run a shovel if the node it was originally running on fails.
</p>
	  ]]></description>
	</item>

	<item>
	  <title>Setting up a Kubernetes Cluster on Vagrant</title>
	  <link>//kubernetes_cluster_vagrant</link>
	  <author>Deepak Gupta</author>
	  <pubDate>2017-04-20T12:30:00+00:00</pubDate>
	  <guid>//kubernetes_cluster_vagrant</guid>
	  <description><![CDATA[
	     In this tutorial we will cover the installation of a Kubernetes Cluster over Vagrant
	  ]]></description>
	</item>

	<item>
	  <title>HTTP Quick Reference - A Quick reminder about HTTP</title>
	  <link>//http_quick_reference</link>
	  <author>Deepak Gupta</author>
	  <pubDate>2017-04-20T12:30:00+00:00</pubDate>
	  <guid>//http_quick_reference</guid>
	  <description><![CDATA[
	     In this tutorial we will cover the basics of an HTTP request and see how it works.
</br>
<h2><u>HTTP Transaction Model:</u></h2>
<p>The HTTP protocol is transaction-driven. This means that each request will lead
to one and only one response. Traditionally, a TCP connection is established
from the client to the server, a request is sent by the client on the
connection, the server responds and the connection is closed. A new request
will involve a new connection :</p>

<code>  [CON1] [REQ1] ... [RESP1] [CLO1] [CON2] [REQ2] ... [RESP2] [CLO2] ... </code>
</br></br>

<p>In this mode, called the "HTTP close" mode, there are as many connection
establishments as there are HTTP transactions. Since the connection is closed
by the server after the response, the client does not need to know the content
length.</p>

<p>Due to the transactional nature of the protocol, it was possible to improve it
to avoid closing a connection between two subsequent transactions. In this mode
however, it is mandatory that the server indicates the content length for each
response so that the client does not wait indefinitely. For this, a special
header is used: "Content-length". This mode is called the "keep-alive" mode :</p>

  <code>[CON] [REQ1] ... [RESP1] [REQ2] ... [RESP2] [CLO] ...</code>
</br></br>
<p>Its advantages are a reduced latency between transactions, and less processing
power required on the server side. It is generally better than the close mode,
but not always because the clients often limit their concurrent connections to
a smaller value.</p>

<p>A last improvement in the communications is the pipelining mode. It still uses
keep-alive, but the client does not wait for the first response to send the
second request. This is useful for fetching large number of images composing a
page :</p>

<code>  [CON] [REQ1] [REQ2] ... [RESP1] [RESP2] [CLO] ...</code>
</br></br>

<p>This can obviously have a tremendous benefit on performance because the network
latency is eliminated between subsequent requests. Many HTTP agents do not
correctly support pipelining since there is no way to associate a response with
the corresponding request in HTTP. For this reason, it is mandatory for the
server to reply in the exact same order as the requests were received.</p>

  <h2><u>HTTP Request</u></h2>
  <p>First, let's consider this HTTP request :</p>
  
  <table id="t01">
  <tr>
    <th>Line Number</th>
    <th>Contents</th> 
  </tr>
  <tr>
    <td>1</td>
    <td>GET /serv/login.php?lang=en&profile=2 HTTP/1.1</td>
  </tr>
  <tr>
    <td>2</td>
    <td>Host: www.mydomain.com</td>
  </tr>
  <tr>
    <td>3</td>
    <td>User-agent: my small browser</td>
  </tr>
  <tr>
    <td>4</td>
    <td>Accept: image/jpeg, image/gif</td>
  </tr>
  <tr>
    <td>5</td>
    <td>Accept: image/png</td>
  </tr>
</table>

<h3><u>The Request line</u></h3>

<p>Line 1 is the "request line". It is always composed of 3 fields :</p>
<li>METHOD      : GET</li>
<li>URI         : /serv/login.php?lang=en&profile=2</li>
<li>Version tag : HTTP/1.1</li>
</br/>
<p>
All of them are delimited by what the standard calls LWS (linear white spaces),
which are commonly spaces, but can also be tabs or line feeds/carriage returns
followed by spaces/tabs. The method itself cannot contain any colon (':') and
is limited to alphabetic letters.
</p>

The URI itself can have several forms :
</br>
<li>A "relative URI" : (<code>/serv/login.php?lang=en&profile=2</code>)
    It is a complete URL without the host part. This is generally what is received by servers, reverse proxies and transparent proxies.
</li>
<li>An "absolute URI", also called a "URL" : (<code>http://192.168.0.12:8080/serv/login.php?lang=en&profile=2</code>)
    It is composed of a "scheme" (the protocol name followed by '://'), a host
    name or address, optionally a colon (':') followed by a port number, then
    a relative URI beginning at the first slash ('/') after the address part.
    This is generally what proxies receive, but a server supporting HTTP/1.1
    must accept this form too.
</li>
<li>A "star" : <code>('*')</code> This form is only accepted in association with the OPTIONS
    method and is not relayable. It is used to inquiry a next hop's capabilities.
</li>
<li>An address:port combination : (<code>192.168.0.12:80</code>)
    This is used with the CONNECT method, which is used to establish TCP
    tunnels through HTTP proxies, generally for HTTPS, but sometimes for
    other protocols too.
</li>
</code>
</br>
<p>
In a relative URI, two sub-parts are identified. The part before the question
mark is called the "path". It is typically the relative path to static objects
on the server. The part after the question mark is called the "query string".
It is mostly used with GET requests sent to dynamic scripts and is very
specific to the language, framework or application in use.
</p>
</br>
<h2><u>The request headers</u></h2>
<p>
The headers start at the second line and are composed of a name at the
beginning of the line, immediately followed by a colon <code>(':')</code>. Traditionally,
an LWS is added after the colon but that's not required. Then come the values.
Multiple identical headers may be folded into one single line, delimiting the
values with commas, provided that their order is respected. This is commonly
encountered in the "Cookie:" field. A header may span over multiple lines if
the subsequent lines begin with an LWS. In the example in 1.2, lines 4 and 5
define a total of 3 values for the "Accept:" header.
</p>

<p>
Contrary to a common mis-conception, header names are not case-sensitive, and
their values are not either if they refer to other header names (such as the
"Connection:" header).
</p>
<p>
The end of the headers is indicated by the first empty line. People often say
that it's a double line feed, which is not exact, even if a double line feed
is one valid form of empty line.
</p>

<h2><u>HTTP response</u></h2>
<p>An HTTP response looks very much like an HTTP request. Both are called HTTP
messages. Let's consider this HTTP response :

<table>
<tr>
<th>Line</th>
<th>Contents</th>
</tr>
<tr>
<td>1</td>
<td>HTTP/1.1 200 OK</td>
</tr>
<tr>
<td>2</td>
<td>Content-length: 350</td>
</tr>
<tr>
<td>3</td>
<td>Content-Type: text/html</td>
</tr>
</table>   
</p>

<p>
As a special case, HTTP supports so called "Informational responses" as status
codes 1xx. These messages are special in that they don't convey any part of the
response, they're just used as sort of a signaling message to ask a client to
continue to post its request for instance.
</p>
<p>
In the case of a status 100 response
the requested information will be carried by the next non-100 response message
following the informational one. This implies that multiple responses may be
sent to a single request, and that this only works when keep-alive is enabled
(1xx messages are HTTP/1.1 only).
</p>

<h2><u>The Response line</u></h2>
<p>Line 1 is the "response line". It is always composed of 3 fields :

<table>
<tr>
<th>Name</th>
<th>Value(can be different at times)</th>
</tr>
<tr>
<td>Version tag</td>
<td>HTTP/1.1</td>
</tr>
<tr>
<td>Status code</td>
<td>200</td>
</tr>
<tr>
<td>Reason</td>
<td>OK</td>
</tr>
</table>

</p>
The status code is always 3-digit. The first digit indicates a general status :
<table>
<tr>
<th>Code</th>
<th>Message</th>
</tr>
<tr>
 <td>1xx</td>
 <td>informational message to be skipped (eg: 100, 101)</td>
</tr>
<tr>
 <td>2xx</td>
 <td>OK, content is following   (eg: 200, 206)</td>
</tr>
<tr>
 <td>3xx</td>
 <td>OK, no content following   (eg: 302, 304)</td>
</tr>
<tr>
 <td>4xx</td>
 <td>error caused by the client (eg: 401, 403, 404)</td>
</tr>
<tr>
 <td>5xx</td>
 <td>error caused by the server (eg: 500, 502, 503)</td>
</tr>
</table>

You can also refer to RFC2616 for the detailed meaning of all such codes. The
"reason" field is just a hint, but is not parsed by clients. Anything can be
found there, but it's a common practice to respect the well-established
messages. It can be composed of one or multiple words, such as "OK", "Found",
or "Authentication Required".

Below is the table depicting the basic interpretation of HTTP status codes:

<table>
<tr>
  <th>Code</th>
  <th>When / reason</th>
</tr>
<tr>
   <td>200</td>
   <td>access to stats page, and when replying to monitoring requests</td>
</tr>
<tr>
   <td>301</td>
   <td>when performing a redirection, depending on the configured code</td>
</tr>
<tr>
   <td>302</td>
   <td>when performing a redirection, depending on the configured code</td>
</tr>
<tr>
   <td>303</td>
   <td>when performing a redirection, depending on the configured code</td>
   </tr>
<tr>
   <td>307</td>
   <td>when performing a redirection, depending on the configured code</td>
   </tr>
<tr>
   <td>308</td>
   <td>when performing a redirection, depending on the configured code</td>
   </tr>
<tr>
   <td>400</td>
   <td>for an invalid or too large request</td>
   </tr>
<tr>
   <td>401</td>
   <td>when an authentication is required to perform the action (when accessing the stats page)</td>
   </tr>
<tr>
   <td>403</td>
   <td>when a request is forbidden by a "block" ACL or "reqdeny" filter</td>
   </tr>
<tr>
   <td>408</td>
   <td>when the request timeout strikes before the request is complete</td>
   </tr>
<tr>
   <td>500</td>
   <td>when haproxy encounters an unrecoverable internal error, such as a memory allocation failure, which should never happen</td>
   </tr>
<tr>
   <td>502</td>
   <td>when the server returns an empty, invalid or incomplete response, or when an "rspdeny" filter blocks the response.</td>
   </tr>
<tr>
   <td>503</td>
   <td>when no server was available to handle the request, or in response to monitoring requests which match the "monitor fail" condition</td>
   </tr>
<tr>
   <td>504</td>
   <td>when the response timeout strikes before the server responds</td>
   </tr>
</table>
</br>

<h2><u>The response headers</u></h2>
<p>Response headers work exactly like request headers.</p>
	  ]]></description>
	</item>

	<item>
	  <title>HA Proxy for Beginners</title>
	  <link>//ha_proxy_for_beginners</link>
	  <author>Deepak Gupta</author>
	  <pubDate>2017-04-20T12:30:00+00:00</pubDate>
	  <guid>//ha_proxy_for_beginners</guid>
	  <description><![CDATA[
	     <h2><u>Introduction</u></h2>

<h5>Installing HA proxy</h5>
<h5>Configuring HAProxy</h5>
<p>
Configuration file format:
</p>

There are 3 major parameters in HA Proxy's configuration:

1. Command Line Arguments, 
HAProxy's configuration process involves 3 major sources of parameters :

  - the arguments from the command-line, which always take precedence
  - the "global" section, which sets process-wide parameters
  - the proxies sections which can take form of "defaults", "listen",
    "frontend" and "backend".

The configuration file syntax consists in lines beginning with a keyword
referenced in this manual, optionally followed by one or several parameters
delimited by spaces. If spaces have to be entered in strings, then they must be
preceded by a backslash ('\') to be escaped. Backslashes also have to be
escaped by doubling them.

	  ]]></description>
	</item>


</channel>
</rss>
