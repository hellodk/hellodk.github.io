<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
   
      <title>http://hellodk.in/</title>
   
   <link>http://hellodk.in/</link>
   <description>A beautiful narrative written over an elegant publishing platform. The story begins here...</description>
   <language>en-uk</language>
   <managingEditor> Deepak Gupta</managingEditor>
   <atom:link href="rss" rel="self" type="application/rss+xml" />
   
	<item>
	  <title>Python regex Simplified</title>
	  <link>//python_regex_simplified</link>
	  <author>Deepak Gupta</author>
	  <pubDate>2017-07-13T09:00:00+05:30</pubDate>
	  <guid>//python_regex_simplified</guid>
	  <description><![CDATA[
	     <h2><u>Introduction</u></h2>

<h3>Python regex metacharacters</h3>
. ^ $ * + ? { } [ ] \ | ( )

<p>

1. [] - 

Negate - ^

Escape Character - \


The first metacharacters we’ll look at are [ and ]. They’re used for specifying a character class, which is a set of characters that you wish to match. Characters can be listed individually, or a range of characters can be indicated by giving two characters and separating them by a '-'. For example, [abc] will match any of the characters a, b, or c; this is the same as [a-c], which uses a range to express the same set of characters. If you wanted to match only lowercase letters, your RE would be [a-z].

Metacharacters are not active inside classes. For example, [akm$] will match any of the characters 'a', 'k', 'm', or '$'; '$' is usually a metacharacter, but inside a character class it’s stripped of its special nature.

You can match the characters not listed within the class by complementing the set. This is indicated by including a '^' as the first character of the class; '^' outside a character class will simply match the '^' character. For example, [^5] will match any character except '5'.

Perhaps the most important metacharacter is the backslash, \. As in Python string literals, the backslash can be followed by various characters to signal various special sequences. It’s also used to escape all the metacharacters so you can still match them in patterns; for example, if you need to match a [ or \, you can precede them with a backslash to remove their special meaning: \[ or \\.

Some of the special sequences beginning with '\' represent predefined sets of characters that are often useful, such as the set of digits, the set of letters, or the set of anything that isn’t whitespace.

Let’s take an example: \w matches any alphanumeric character. If the regex pattern is expressed in bytes, this is equivalent to the class [a-zA-Z0-9_]. If the regex pattern is a string, \w will match all the characters marked as letters in the Unicode database provided by the unicodedata module. You can use the more restricted definition of \w in a string pattern by supplying the re.ASCII flag when compiling the regular expression.

The following list of special sequences isn’t complete. For a complete list of sequences and expanded class definitions for Unicode string patterns, see the last part of Regular Expression Syntax in the Standard Library reference. In general, the Unicode versions match any character that’s in the appropriate category in the Unicode database.

\d
Matches any decimal digit; this is equivalent to the class [0-9].
\D
Matches any non-digit character; this is equivalent to the class [^0-9].
\s
Matches any whitespace character; this is equivalent to the class [ \t\n\r\f\v].
\S
Matches any non-whitespace character; this is equivalent to the class [^ \t\n\r\f\v].
\w
Matches any alphanumeric character; this is equivalent to the class [a-zA-Z0-9_].
\W
Matches any non-alphanumeric character; this is equivalent to the class [^a-zA-Z0-9_].
These sequences can be included inside a character class. For example, [\s,.] is a character class that will match any whitespace character, or ',' or '.'.

The final metacharacter in this section is .. It matches anything except a newline character, and there’s an alternate mode (re.DOTALL) where it will match even a newline. '.' is often used where you want to match “any character”.
</p>
	  ]]></description>
	</item>

	<item>
	  <title>Mac Shortcuts</title>
	  <link>//mac_shortcuts</link>
	  <author>Deepak Gupta</author>
	  <pubDate>2017-07-05T15:48:00+05:30</pubDate>
	  <guid>//mac_shortcuts</guid>
	  <description><![CDATA[
	     <h3>Keyboard Tips</h3>
<ol>
<li>The Delete key works like Backspace if you are from other platforms. To delete on the other direction, press Fn + Delete</li>
<li>The Backspace will feel slow by default. This can be changed in Keyboard Preferences by making "Key Repeat" fast & "Delay Until Repeat" short. Ref: http://forums.macrumors.com/showthread.php?t=508385 </li>
<li>If you are missing Home & End keys, it is Command+ Left Arrow & Command + Right Arrow</li>
<li>If you are missing Page Up & Down keys, it is Command+ Up Arrow & Command + Down Arrow</li>
 
<h3>Mouse/Trackpad Tips:</h3>
To Drag something, Select Text, you can press the mouse button and drag it. If you are familiar with Double Tap & Drag like in Windows, Linux, You can have the same settings enabled in System Preferences > Universal Access > Mouse & Trackpad >Trackpad Options -> Enable Dragging Without Lock. Ref: http://chris.dziemborowicz.com/blog/2012/07/04/enable-double-tap-to-drag-in-mac-os-x-lion/
 
Navigation:
Command+Down To Open a Folder/File or to launch an application from keyboard
Command + Up   To go one level up in finder
Ctrl + Up             To Bring Mission Control
Command + tilde  To Switch windows of same App
Command + h       To hide the window
/                          To enter/paste a path in Finder Window (Open, Save Dialog boxes)
 
Terminal:
Ctrl+a  To go to the beginning of line, Ctrl+e to go the end of line. The Link has more shortcuts: http://apple.stackexchange.com/questions/12997/can-home-and-end-keys-be-mapped-when-using-terminal
Tweaks:
Show Full Path in the title bar:
defaults write com.apple.finder _FXShowPosixPathInTitle -bool YES
killall Finder
 Ref: http://osxdaily.com/2007/12/02/show-full-directory-path-in-finder-window-title-bars/
Copy Path of selected Folder/File:
Ref: http://osxdaily.com/2013/06/19/copy-file-folder-path-mac-os-x/
	  ]]></description>
	</item>

	<item>
	  <title>Yum Commands - A quick jist</title>
	  <link>//yum_commands_quick_reference</link>
	  <author>Deepak Gupta</author>
	  <pubDate>2017-06-10T17:50:00+05:30</pubDate>
	  <guid>//yum_commands_quick_reference</guid>
	  <description><![CDATA[
	     <ol>
<li><h5>To make a search of some package or term in the data base in some of the formed deposits yum in the system:</h5></li>
</br>
<p>Syntax: <span style="font-family: courier new,courier;">yum search any-package</span></p>
Example: <code>yum search httpd</code>
<h5>To consult the information contained in a package in individual:</h5>
</br>yum info any-package
</br>Example: yum info httpd
</br></br>Uninstalling packages. Desinstalación of packages along with everything what it depends on these:
</br>yum remove any-package
</br>Example: yum remove gkrellm
</br>The following thing will list all the packages available in the data base yum and that can settle:
</br>available yum list|less
</br>The following thing will list all the packages installed in the system:
</br>yum list installed|less
</br>The following thing will list all the packages installed in the system and that can (they must) be updated:
</br>yum list updates|less
</br>Cleaning of the system.
</br>
<p><span style="font-family: courier new,courier;">Yum leaves as result of its use heads and packages RPM stored in the interior of the directory located in the route /var/cache/yum/. Particularly the packages RPM that have settled can occupy much space and is by such reason agrees to eliminate them once no longer they have utility. Also it agrees to do the same with the old heads of packages that no longer are in the data base. In order to make the </br>corresponding cleaning, the following thing can be executed:</span></p>s
</br>
</br><code>yum clean all</code>
</br>Group install:
</br><code>yum groupinstall "groupname"</code>
</ol>
	  ]]></description>
	</item>

	<item>
	  <title>Configure multiple SSH identities for GitBash, Mac OSX, & Linux</title>
	  <link>//configure_multiple_ssh_identities_for_gitbash_mac_linux</link>
	  <author>Deepak Gupta</author>
	  <pubDate>2017-04-26T18:48:00+05:30</pubDate>
	  <guid>//configure_multiple_ssh_identities_for_gitbash_mac_linux</guid>
	  <description><![CDATA[
	     <p>
</p>
	  ]]></description>
	</item>

	<item>
	  <title>RabbitMQ - Detecting Dead TCP Connections with Heartbeats</title>
	  <link>//detecting-dead-tcp-connections</link>
	  <author>Deepak Gupta</author>
	  <pubDate>2017-04-24T17:48:00+05:30</pubDate>
	  <guid>//detecting-dead-tcp-connections</guid>
	  <description><![CDATA[
	     <p>
In some types of network failure, packet loss can mean that disrupted TCP connections take a moderately long time (about 11 minutes with default configuration on Linux, for example) to be detected by the operating system. AMQP 0-9-1 offers a heartbeat feature to ensure that the application layer promptly finds out about disrupted connections (and also completely unresponsive peers). Heartbeats also defend against certain network equipment which may terminate "idle" TCP connections. See Heartbeats for details.
</p>

<h4><u>At the Broker</u></h4>
<p>
In order to avoid losing messages in the broker we need to cope with broker restarts, broker hardware failure and in extremis even broker crashes.
</p>
<p>
To ensure that messages and broker definitions survive restarts, we need to ensure that they are on disk. The AMQP standard has a concept of durability for exchanges, queues and of persistent messages, requiring that a durable object or persistent message will survive a restart. More details about specific flags pertaining to durability and persistence can be found in the AMQP Concepts Guide.
</p>
<h4><u>Clustering and High Availability</u></h4>
<p>
If we need to ensure that our broker survives hardware failure, we can use RabbitMQ's clustering. In a RabbitMQ cluster, all definitions (of exchanges, bindings, users, etc) are mirrored across the entire cluster. Queues behave differently, by default residing only on a single node, but optionally being mirrored across several or all nodes. Queues remain visible and reachable from all nodes regardless of where they are located.
</p>
<p>
Mirrored queues replicate their contents across all configured cluster nodes, tolerating node failures seamlessly and without message loss (although see this note on unsynchronised slaves). However, consuming applications need to be aware that when queues fail their consumers will be cancelled and they will need to reconsume - see the documentation for more details.
</p>
At the Producer
<p>
When using confirms, producers recovering from a channel or connection failure should retransmit any messages for which an acknowledgement has not been received from the broker. There is a possibility of message duplication here, because the broker might have sent a confirmation that never reached the producer (due to network failures, etc). Therefore consumer applications will need to perform deduplication or handle incoming messages in an idempotent manner.
</p>
<h4><u>Ensuring Messages are Routed</u></h4>
<p>
In some circumstances it can be important for producers to ensure that their messages are being routed to queues (although not always - in the case of a pub-sub system producers will just publish and if no consumers are interested it is correct for messages to be dropped).
</p>
<p>
To ensure messages are routed to a single known queue, the producer can just declare a destination queue and publish directly to it. If messages may be routed in more complex ways but the producer still needs to know if they reached at least one queue, it can set the mandatory flag on a basic.publish, ensuring that a basic.return (containing a reply code and some textual explanation) will be sent back to the client if no queues were appropriately bound.
</p>
<p>
Producers should also be aware that when publishing to a clustered node, if one or more destination queues that are bound to the exchange have mirrors in the cluster, it's possible to incur delays in the face of network failures between nodes, due to flow control between replicas and the master queue process. See here for more details.
</p>
<h4><u>At the Consumer</u></h4>
<p>
In the event of network failure (or a node crashing), messages can be duplicated, and consumers must be prepared to handle them. If possible, the simplest way to handle this is to ensure that your consumers handle messages in an idempotent way rather than explicitly deal with deduplication.
</p>
<p>
If a message is delivered to a consumer and then requeued (because it was not acknowledged before the consumer connection dropped, for example) then RabbitMQ will set the redelivered flag on it when it is delivered again (whether to the same consumer or a different one). This is a hint that a consumer may have seen this message before (although that's not guaranteed, the message may have made it out of the broker but not into a consumer before the connection dropped). Conversely if the redelivered flag is not set then it is guaranteed that the message has not been seen before. Therefore if a consumer finds it more expensive to deduplicate messages or process them in an idempotent manner, it can do this only for messages with the redelivered flag set.
</p>
<h4><u>Consumer Cancel Notification</u></h4>
<p>
Under some circumstances the server needs to be able to cancel a consumer - since the queue it was consuming from has been deleted, or has failed over. In this case the consumer should consume again but be aware that it may see messages again which it has already seen.
</p>
<code>Note that consumer cancel notification is a RabbitMQ extension to AMQP, and as such may not be supported by all clients.</code>
</br></br>
<h4><u>Messages That Cannot Be Processed</u></h4>
<p>
If a consumer determines that it cannot handle a message then it can reject it using basic.reject (or basic.nack), either asking the server to requeue it, or not (in which case the server might be configured to dead-letter it instead.
</p>
<h4><u>Distributed RabbitMQ</u></h4>
<p>
Rabbit provides two plugins to assist with distributing nodes over unreliable networks: federation and the shovel. Both are implemented as AMQP clients, so if you configure them to use confirms and acknowledgements, they will retransmit when necessary. Both will use confirms and acknowledgements by default.
</p>
<p>
When connecting clusters with federation or the shovel, it is desirable to ensure that the federation links and shovels tolerate node failures. Federation will automatically distribute links across the downstream cluster and fail them over on failure of a downstream node. In order to connect to a new upstream when an upstream node fails you can specify multiple redundant URIs for an upstream, or connect via a TCP load balancer.
</p>
<p>
When using the shovel, it is possible to specify redundant brokers in a source or destination clause; however it is not currently possible to make the shovel itself redundant. We hope to improve this situation in the future; in the mean time a new node can be brought up manually to run a shovel if the node it was originally running on fails.
</p>
	  ]]></description>
	</item>

	<item>
	  <title>HTTP Quick Reference - A Quick reminder about HTTP</title>
	  <link>//http_quick_reference</link>
	  <author>Deepak Gupta</author>
	  <pubDate>2017-04-20T18:00:00+05:30</pubDate>
	  <guid>//http_quick_reference</guid>
	  <description><![CDATA[
	     In this tutorial we will cover the basics of an HTTP request and see how it works.
</br>
<h2><u>HTTP Transaction Model:</u></h2>
<p>The HTTP protocol is transaction-driven. This means that each request will lead
to one and only one response. Traditionally, a TCP connection is established
from the client to the server, a request is sent by the client on the
connection, the server responds and the connection is closed. A new request
will involve a new connection :</p>

<code>  [CON1] [REQ1] ... [RESP1] [CLO1] [CON2] [REQ2] ... [RESP2] [CLO2] ... </code>
</br></br>

<p>In this mode, called the "HTTP close" mode, there are as many connection
establishments as there are HTTP transactions. Since the connection is closed
by the server after the response, the client does not need to know the content
length.</p>

<p>Due to the transactional nature of the protocol, it was possible to improve it
to avoid closing a connection between two subsequent transactions. In this mode
however, it is mandatory that the server indicates the content length for each
response so that the client does not wait indefinitely. For this, a special
header is used: "Content-length". This mode is called the "keep-alive" mode :</p>

  <code>[CON] [REQ1] ... [RESP1] [REQ2] ... [RESP2] [CLO] ...</code>
</br></br>
<p>Its advantages are a reduced latency between transactions, and less processing
power required on the server side. It is generally better than the close mode,
but not always because the clients often limit their concurrent connections to
a smaller value.</p>

<p>A last improvement in the communications is the pipelining mode. It still uses
keep-alive, but the client does not wait for the first response to send the
second request. This is useful for fetching large number of images composing a
page :</p>

<code>  [CON] [REQ1] [REQ2] ... [RESP1] [RESP2] [CLO] ...</code>
</br></br>

<p>This can obviously have a tremendous benefit on performance because the network
latency is eliminated between subsequent requests. Many HTTP agents do not
correctly support pipelining since there is no way to associate a response with
the corresponding request in HTTP. For this reason, it is mandatory for the
server to reply in the exact same order as the requests were received.</p>

  <h2><u>HTTP Request</u></h2>
  <p>First, let's consider this HTTP request :</p>
  
  <table id="t01">
  <tr>
    <th>Line Number</th>
    <th>Contents</th> 
  </tr>
  <tr>
    <td>1</td>
    <td>GET /serv/login.php?lang=en&profile=2 HTTP/1.1</td>
  </tr>
  <tr>
    <td>2</td>
    <td>Host: www.mydomain.com</td>
  </tr>
  <tr>
    <td>3</td>
    <td>User-agent: my small browser</td>
  </tr>
  <tr>
    <td>4</td>
    <td>Accept: image/jpeg, image/gif</td>
  </tr>
  <tr>
    <td>5</td>
    <td>Accept: image/png</td>
  </tr>
</table>

<h3><u>The Request line</u></h3>

<p>Line 1 is the "request line". It is always composed of 3 fields :</p>
<li>METHOD      : GET</li>
<li>URI         : /serv/login.php?lang=en&profile=2</li>
<li>Version tag : HTTP/1.1</li>
</br/>
<p>
All of them are delimited by what the standard calls LWS (linear white spaces),
which are commonly spaces, but can also be tabs or line feeds/carriage returns
followed by spaces/tabs. The method itself cannot contain any colon (':') and
is limited to alphabetic letters.
</p>

The URI itself can have several forms :
</br>
<li>A "relative URI" : (<code>/serv/login.php?lang=en&profile=2</code>)
    It is a complete URL without the host part. This is generally what is received by servers, reverse proxies and transparent proxies.
</li>
<li>An "absolute URI", also called a "URL" : (<code>http://192.168.0.12:8080/serv/login.php?lang=en&profile=2</code>)
    It is composed of a "scheme" (the protocol name followed by '://'), a host
    name or address, optionally a colon (':') followed by a port number, then
    a relative URI beginning at the first slash ('/') after the address part.
    This is generally what proxies receive, but a server supporting HTTP/1.1
    must accept this form too.
</li>
<li>A "star" : <code>('*')</code> This form is only accepted in association with the OPTIONS
    method and is not relayable. It is used to inquiry a next hop's capabilities.
</li>
<li>An address:port combination : (<code>192.168.0.12:80</code>)
    This is used with the CONNECT method, which is used to establish TCP
    tunnels through HTTP proxies, generally for HTTPS, but sometimes for
    other protocols too.
</li>
</code>
</br>
<p>
In a relative URI, two sub-parts are identified. The part before the question
mark is called the "path". It is typically the relative path to static objects
on the server. The part after the question mark is called the "query string".
It is mostly used with GET requests sent to dynamic scripts and is very
specific to the language, framework or application in use.
</p>
</br>
<h2><u>The request headers</u></h2>
<p>
The headers start at the second line and are composed of a name at the
beginning of the line, immediately followed by a colon <code>(':')</code>. Traditionally,
an LWS is added after the colon but that's not required. Then come the values.
Multiple identical headers may be folded into one single line, delimiting the
values with commas, provided that their order is respected. This is commonly
encountered in the "Cookie:" field. A header may span over multiple lines if
the subsequent lines begin with an LWS. In the example in 1.2, lines 4 and 5
define a total of 3 values for the "Accept:" header.
</p>

<p>
Contrary to a common mis-conception, header names are not case-sensitive, and
their values are not either if they refer to other header names (such as the
"Connection:" header).
</p>
<p>
The end of the headers is indicated by the first empty line. People often say
that it's a double line feed, which is not exact, even if a double line feed
is one valid form of empty line.
</p>

<h2><u>HTTP response</u></h2>
<p>An HTTP response looks very much like an HTTP request. Both are called HTTP
messages. Let's consider this HTTP response :

<table>
<tr>
<th>Line</th>
<th>Contents</th>
</tr>
<tr>
<td>1</td>
<td>HTTP/1.1 200 OK</td>
</tr>
<tr>
<td>2</td>
<td>Content-length: 350</td>
</tr>
<tr>
<td>3</td>
<td>Content-Type: text/html</td>
</tr>
</table>   
</p>

<p>
As a special case, HTTP supports so called "Informational responses" as status
codes 1xx. These messages are special in that they don't convey any part of the
response, they're just used as sort of a signaling message to ask a client to
continue to post its request for instance.
</p>
<p>
In the case of a status 100 response
the requested information will be carried by the next non-100 response message
following the informational one. This implies that multiple responses may be
sent to a single request, and that this only works when keep-alive is enabled
(1xx messages are HTTP/1.1 only).
</p>

<h2><u>The Response line</u></h2>
<p>Line 1 is the "response line". It is always composed of 3 fields :

<table>
<tr>
<th>Name</th>
<th>Value(can be different at times)</th>
</tr>
<tr>
<td>Version tag</td>
<td>HTTP/1.1</td>
</tr>
<tr>
<td>Status code</td>
<td>200</td>
</tr>
<tr>
<td>Reason</td>
<td>OK</td>
</tr>
</table>

</p>
The status code is always 3-digit. The first digit indicates a general status :
<table>
<tr>
<th>Code</th>
<th>Message</th>
</tr>
<tr>
 <td>1xx</td>
 <td>informational message to be skipped (eg: 100, 101)</td>
</tr>
<tr>
 <td>2xx</td>
 <td>OK, content is following   (eg: 200, 206)</td>
</tr>
<tr>
 <td>3xx</td>
 <td>OK, no content following   (eg: 302, 304)</td>
</tr>
<tr>
 <td>4xx</td>
 <td>error caused by the client (eg: 401, 403, 404)</td>
</tr>
<tr>
 <td>5xx</td>
 <td>error caused by the server (eg: 500, 502, 503)</td>
</tr>
</table>

You can also refer to RFC2616 for the detailed meaning of all such codes. The
"reason" field is just a hint, but is not parsed by clients. Anything can be
found there, but it's a common practice to respect the well-established
messages. It can be composed of one or multiple words, such as "OK", "Found",
or "Authentication Required".

Below is the table depicting the basic interpretation of HTTP status codes:

<table>
<tr>
  <th>Code</th>
  <th>When / reason</th>
</tr>
<tr>
   <td>200</td>
   <td>access to stats page, and when replying to monitoring requests</td>
</tr>
<tr>
   <td>301</td>
   <td>when performing a redirection, depending on the configured code</td>
</tr>
<tr>
   <td>302</td>
   <td>when performing a redirection, depending on the configured code</td>
</tr>
<tr>
   <td>303</td>
   <td>when performing a redirection, depending on the configured code</td>
   </tr>
<tr>
   <td>307</td>
   <td>when performing a redirection, depending on the configured code</td>
   </tr>
<tr>
   <td>308</td>
   <td>when performing a redirection, depending on the configured code</td>
   </tr>
<tr>
   <td>400</td>
   <td>for an invalid or too large request</td>
   </tr>
<tr>
   <td>401</td>
   <td>when an authentication is required to perform the action (when accessing the stats page)</td>
   </tr>
<tr>
   <td>403</td>
   <td>when a request is forbidden by a "block" ACL or "reqdeny" filter</td>
   </tr>
<tr>
   <td>408</td>
   <td>when the request timeout strikes before the request is complete</td>
   </tr>
<tr>
   <td>500</td>
   <td>when haproxy encounters an unrecoverable internal error, such as a memory allocation failure, which should never happen</td>
   </tr>
<tr>
   <td>502</td>
   <td>when the server returns an empty, invalid or incomplete response, or when an "rspdeny" filter blocks the response.</td>
   </tr>
<tr>
   <td>503</td>
   <td>when no server was available to handle the request, or in response to monitoring requests which match the "monitor fail" condition</td>
   </tr>
<tr>
   <td>504</td>
   <td>when the response timeout strikes before the server responds</td>
   </tr>
</table>
</br>

<h2><u>The response headers</u></h2>
<p>Response headers work exactly like request headers.</p>
	  ]]></description>
	</item>

	<item>
	  <title>HA Proxy for Beginners</title>
	  <link>//ha_proxy_for_beginners</link>
	  <author>Deepak Gupta</author>
	  <pubDate>2017-04-20T18:00:00+05:30</pubDate>
	  <guid>//ha_proxy_for_beginners</guid>
	  <description><![CDATA[
	     <h2><u>Introduction</u></h2>

<h5>Installing HA proxy</h5>
<h5>Configuring HAProxy</h5>
<p>
Configuration file format:
</p>

There are 3 major parameters in HA Proxy's configuration:

1. Command Line Arguments, 
HAProxy's configuration process involves 3 major sources of parameters :

  - the arguments from the command-line, which always take precedence
  - the "global" section, which sets process-wide parameters
  - the proxies sections which can take form of "defaults", "listen",
    "frontend" and "backend".

The configuration file syntax consists in lines beginning with a keyword
referenced in this manual, optionally followed by one or several parameters
delimited by spaces. If spaces have to be entered in strings, then they must be
preceded by a backslash ('\') to be escaped. Backslashes also have to be
escaped by doubling them.

	  ]]></description>
	</item>

	<item>
	  <title>RethinkDb Installation on Ubuntu-14.04</title>
	  <link>//rethinkdb_installation_ubuntu14</link>
	  <author>Deepak Gupta</author>
	  <pubDate>2016-09-21T15:48:00+05:30</pubDate>
	  <guid>//rethinkdb_installation_ubuntu14</guid>
	  <description><![CDATA[
	     Let's get some hands on rethinkdb today and find it out yourself. </br>
</br>
So what is rethinkdb?
</br></br>
Rethinkdb is an open-source, scalable JSON database built from the ground up for the realtime web.
</br>RethinkDB inverts the traditional database architecture by exposing an exciting new access model – instead of polling for changes, the developer can tell RethinkDB to continuously push updated query results to applications in realtime. RethinkDB’s realtime push architecture dramatically reduces the time and effort necessary to build scalable realtime apps. RethinkDB also offers a flexible query language, intuitive operations and monitoring APIs, and is easy to setup and learn. Just like any other database solution, rethinkdb ships as a client-server component model. The installation process for both the server and the client are illustrated below:

<p>How to Install RethinkDb:</p>
1. Add the RethinkDB PPA to your list of repositories : 
<br></br>
<code>source /etc/lsb-release && echo "deb http://download.rethinkdb.com/apt $DISTRIB_CODENAME main" | sudo tee /etc/apt/sources.list.d/rethinkdb.list</code>
<br></br>
2. Add the keys:<br></br>

<code>wget -qO- http://download.rethinkdb.com/apt/pubkey.gpg | sudo apt-key add -</code>
<br></br>
3. Update the repository:
<br></br>
<code>sudo apt-get update</code>
<br></br>
4. Install the rethinkdb server via apt-get:
<br></br>
<code>sudo apt-get -y install rethinkdb</code>
<br></br>
Install rethinkdb client:
<br></br>
1. Install the python-pip package:
<br></br>
<code>sudo apt-get install python-pip</code>
<br></br>
2. Install the rethinkdb python client:
<br></br>
<code>sudo pip install rethinkdb</code>
<br></br>
The above steps ensure that rethinkdb is installed on the system, while it does not ensures that this will start the rethinkdb service on system startup. You still need to start the rethinkdb service using the below command
<br></br>
<code>rethinkdb</code>
<br></br>
The above command will ensure that rethinkdb is running as a terminal process, and will exit once the terminal is closed, or the process is killed, in short it will not run rethinkdb as a background service.
<br></br>
To start rethinkdb as a service, please follow the below steps:
<br></br>
1.  Move to the directory /etc/rethinkdb
<br></br><code>cd /etc/rethinkdb</code>
<br></br>
2. Copy the file to /etc/rethinkdb/instances.d and rename the file as per your requirements ensuring the extension is .conf only. Say for example the file name is rethinkdb1.conf
<br></br><code>cp default.conf.sample rethinkdb1.conf</code>
<br></br>
3. Now open the file /etc/rethinkdb/instances.d/rethinkdb1.conf and modify the paramaters as per your requirements.
<br></br><code>vim rethinkdb1.conf</code>
<br></br>
4. If setting up a cluster, we suggest do change the server-name to somethink like 'rethinkdb-primary' or 'rethinkdb-1' or 'master' or 'slave'. This will ensure that we have a meaningful naming convention for our cluster.
<br></br>
5. The default port details are :
<br></br>
<code>29015 :</code> Rethinkdb listens for intracluster connections
</br>
<code>28015 :</code> Rethinkdb listens for client driver connections
</br>
<code>8080  :</code> Rethinkdb listens for administrative HTTP connections
</br>
<code>22    :</code> For SSH. The server uses public key authentication.
</br>
<code>80    :</code> For HTTP. It is used during the setup process but otherwise redirects to HTTPS.
</br>
<code>443   :</code> For HTTPS. An Nginx server sits between RethinkDB and the world and provides basic HTTP authentication and secure HTTPS connections for the web UI
<br></br>
</body>
</html>
	  ]]></description>
	</item>

	<item>
	  <title>MySQL multi master replication fundas</title>
	  <link>//mysql_master_master_replication_insights</link>
	  <author>Deepak Gupta</author>
	  <pubDate>2016-09-21T15:48:00+05:30</pubDate>
	  <guid>//mysql_master_master_replication_insights</guid>
	  <description><![CDATA[
	     <html>
<p>
Master-master replication (more generally -- multi-master replication) conceptually works by assuming that conflicts are not common and only keeping the entire system loosely consistent, asynchonously communication updates between masters, which ends up violating basic ACID properties.

As carmbrester correctly suggested (and independent of MySQL), key generation is an important mitigation strategy against conflicts (e.g., same ID being generated). Beyond that, I think the key is to think of conflicts in multi-master systems as akin to conflicts in optimistic locking models. Both succeed because the remaining risks are generally small compared to the payoffs.

However, conflicts can happen (as you suggest) and different vendors provide different strategies. For example Oracle allows DBAs to choose from a range of conflict resolution basis-es (timestamp, affinity, etc) and subsqeuent actions (logging an error in a queue, etc). See more details on Oracle at http://www.orafaq.com/wiki/Advanced_Replication_FAQ#What_happens_if_two_or_more_sites_change_the_same_data.3F
</p>

<p>Context is important, but here is a little bit of information on how MySql handles the scenario...

The binlog from each master is read and executed on the other master. Auto-increment offset is configured so that primary keys do not collide, i.e. one of the masters would be configured with an offset that results in even numbers are used during auto-increment, while the other master would be configured to use odd numbers during auto-increment.

For MySql Clustering replication, which can also be configured with dual masters, you can configure how conflicts are handled as outlined in this link.</p>
</html>
	  ]]></description>
	</item>

	<item>
	  <title>The UNIX Time-Sharing System</title>
	  <link>//unix_time_sharing_system</link>
	  <author>Deepak Gupta</author>
	  <pubDate>2016-02-01T13:48:37+05:30</pubDate>
	  <guid>//unix_time_sharing_system</guid>
	  <description><![CDATA[
	     <div class="highlight"><pre><code class="language-text" data-lang="text">                          Drafted By

                        D. M. Ritchie
</code></pre></div>
<h3>Introduction</h3>

<p>UNIX is a general-purpose, multi-user time sharing system implemented on several Digital Equipment Corporation PDP series machines.</p>

<p>UNIX was written by K. L. Thompson, who also wrote many of the command programs. The author of this memorandum contributed several of the major commands, including the assembler and the debugger. The file system was originally designed by Thompson, the author, and R. H. Canaday.</p>

<p>There are two versions of UNIX. The first, which has been in existence about a year, runs on the PDP-7 and -9 computers; a more modern version, a few months old, uses the PDP-11. This document describes UNIX-11, since it is more modern and many of the differences between it and UNIX-7 result from redesign of features found to be deficient or lacking in the earlier system. Although the PDP-7 and PDP-11 are both small computers, the design of UNIX is amenable to expansion for use on more powerful machines. Indeed, UNIX contains a number of features very seldom offered even by larger systems, including</p>

<p>A versatile, convenient file system with complete integration between disk files and I/O devices;</p>

<p>The ability to initiate asynchrously running processes.</p>

<p>It must be said, however, that the most important features of UNIX are its simplicity, elegance, and ease of use.</p>

<p>Besides the system proper, the major programs available under UNIX are an assembler, a text editor based on QED, a symbolic debugger for examining and patching faulty programs, and &quot;B&quot;, a higher level language resembling BCPL. UNIX-7 also has a version of the compiler writing language TMGL contributed by M. D. McIlroy, and besides its own assembler, there is a PDP-11 assembler which was used to write UNIX-11. On the PDP-11 there is a version of BASIC [reference] adapted from the one supplied by DEC [reference]. All but the last of these programs were written locally, and except for the very first versions of the editor and assembler, using UNIX itself.</p>

<p>Hardware
The PDP-11 on which UNIX is implemented is a 16-bit 12K computer, and UNIX occupies 8K words. More than half of this space, however, is utilized for a variable number of disk buffers; with some loss of speed the number of buffers could be cut significantly.</p>

<p>The PDP-11 has a 256K word disk, almost all of which is used for file system storage. It is equipped with DECTAPE, a variety of magnetic tape facility in which individual records may be addressed and rewritten at will. Also available are a high-speed paper tape reader and punch. Besides the standard Teletype, there are several variable-speed communications interfaces.</p>

<p>The File System
The most important role of UNIX is to provide a file system. From the point of view of the user, there are three kinds of files: ordinary disk files, directories, and special files.</p>

<h4>Ordinary Files</h4>

<p>A file contains whatever information the user places there, for example symbolic or binary (object) programs. No particular structuring is expected by the system. Files of text ordinarily consist simply of a string of characters, with lines demarcated by the new-line character. Binary programs are sequences of words as they will appear in core memory when the program starts executing. A few user programs generate and expect files with more structure; for example, the assembler generates, and the debugger expects, a name list file in a particular format; however, the structure of files is controlled solely by the programs which use them, not by the system.</p>

<h4>Directories</h4>

<p>Directories (sometimes, &quot;catalogs&quot;), provide the mapping between the names of files and the files themselves, and thus induce a structure on the file system as a whole. Each user has a directory of his own files; he may also create subdirectories to contain groups of files conveniently treated together.</p>

<p>A directory is exactly like an ordinary file except that it cannot be written on by user programs, so that the system controls the contents of directories. However, anyone with appropriate permission may read a directory just like any other file.</p>

<p>The system maintains several directories for its own use. One of these is the root directory. All files in the system can be found by tracing a path through a chain of directories until the desired file is reached. The starting point for such searches is often the root, which contains an entry for each user&#39;s master directory. Another system directory contains all the programs provided as part of the system; that is, all the commands (elsewhere, &quot;subsystems&quot;). As will be seen, however, it is by no means necessary that a program reside in this directory for it to be used as a command.</p>

<p>Files and directories are named by sequences of eight or fewer characters. When the name of a file is specified to the system, it may be in the form of a path name, which is a sequence of directory names separated by slashes and ending in a file name. If the sequence begins with a slash, the search begins in the root directory. The name &quot;/a/b/c&quot; causes the system to search the root for directory &quot;a&quot;; then to search &quot;a&quot; for &quot;b&quot;, and then to find &quot;c&quot; in &quot;b&quot;. &quot;c&quot; may be an ordinary file, a directory, or a special file. As a limiting case, the name &quot;/&quot; refers to the root itself.</p>

<p>The same non-directory file may appear in several directories under possibly different names. This feature is called &quot;linking&quot;; a directory entry for a file is sometimes called a link. UNIX differs from other systems in which linking is permitted in that all links to a file have equal status. That is, a file does not exist within a particular directory; the directory entry for a file consists merely of its name and a pointer to the information actually describing the file. Thus a file exists independently of any directory entry, although in practice a file is made to disappear along with the last link to it.</p>

<p>When a user logs into UNIX, he is assigned a default current directory, but he may change to any directory readable by him. A path name not starting with &quot;/&quot; causes the system to begin the search in the userâ€™s current directory. Thus, the name &quot;a/b&quot; specifies the file named &quot;b&quot; in directory &quot;a&quot;, which is found in the current working directory. The simplest kind of name, for example &quot;a&quot;, refers to a file which itself is found in the working directory.</p>

<p>Each directory always has at least two entries. The name &quot;.&quot; in each directory refers to the directory itself. Thus a program may read the current directory under the name &quot;.&quot; without knowing its actual path name. The name &quot;..&quot; by convention refers to the parent of the directory in which it appears; that is, the directory in which it was first created.</p>

<p>The directory structure is constrained to have the form of a rooted tree. Except for the special entries &quot;.&quot; and &quot;..&quot;, each directory must appear as an entry in exactly one other, which is its parent. The reason for this is to simplify the writing of programs which visit subtrees of the directory structure, and more important, to avoid the separation of portions of the hierarchy. If arbitrary links to directories were permitted, it would be quite difficult to detect when the last connection from the root to a directory was severed.</p>

<h4>Special Files</h4>

<p>Special files constitute the most unusual feature of the UNIX file system. Each I/O device supported by UNIX is associated with at least one special file. Special files are read and written just like ordinary disk files, but the result is activation of the associated device. Entries for all special files reside in the root directory, so they may all be referred to by &quot;/&quot; followed by the appropriate name.</p>

<p>The special files are discussed further in section 6 below.</p>

<h4>Protection</h4>

<p>The protection scheme in UNIX is quite simple. Each user of the system is assigned a unique user number. When a file-is created, it is marked with the number of its creator. Also given for new files is a set of protection bits. Four of these specify independently permission to read or write for the owner of the file and for all other users. A fifth bit indicates permission to execute the file as a program. If the sixth bit is on, the system will temporarily change the user identification of the current user to that of the creator of the file whenever the file is executed as a program. This feature provides for privileged programs which may use files which should neither be read nor changed by other users. If the set-user-identification bit is on for a program, the accounting file may be accessed during the programâ€™s execution but not otherwise.</p>

<h4>System I/O Calls</h4>

<p>The system calls to do I/O are designed to eliminate the differences between the various devices and styles of access. There is no distinction between &quot;random&quot; and sequential I/O, nor is any logical or physical record size imposed by the system. The size of a file on the disk is determined by the location of the last piece of information written on it; no predetermination of the size of a file is necessary. In UNIX-11, the unit of information is the 8-bit byte, since the PDP-11 is a byte-oriented machine.</p>

<p>To illustrate the essentials of I/O in UNIX, the basic calls are summarized below in an anonymous higher level language which will indicate the needed parameters without getting into the complexities of machine language programming. (All system calls are also described in Appendix 1 in their actual form.) Each call to the system may potentially result in an error return, which for simplicity is not represented in the calling sequence.</p>

<h5>Open</h5>

<p>To read or write a file assumed to exist already, it must be Opened by the following call:</p>

<p>filep = open(name, flag)
Name indicates the name of the file. An arbitrary path name may be given. The flag argument indicates whether the file is to be read or written. If the file is to be &quot;updated&quot;, that is read and written simultaneously, it may be opened twice, once for reading and once for writing.</p>

<p>The returned argument filep is called a file descriptor. It is used to identify the file in subsequent calls to read, write or otherwise manipulate the file.</p>

<p>There are no locks in the file system, nor is there any restriction on the number of users who may have a file open for reading or writing. Although one may imagine situations in which this fact is unfortunate, in practice difficulties are quite rare.</p>

<h5>Create</h5>

<p>To create a new file, the following call is used.</p>

<p>filep = create(name, mode)
Here filep and name are as before. If the file already existed, it is truncated to zero length. Creation of a file implies opening for writing as well. The mode argument indicates the permissions which are to be placed on the file by the protection mechanism. To create a file, the user must have write permission in the directory in which the file is being created.</p>

<h5>Write</h5>

<p>Except as indicated below, reading and writing are sequential. This means that if a particular byte in the file was the last byte written (or read), the next I/O call implicitly refers to the first following byte. For each Open file there is a pointer, maintained by the system, which always indicates the next byte to be read or written. If n bytes are read, the pointer advances by n bytes.</p>

<p>Once a file is open for writing, the following call may be used.</p>

<p>nwritten = write(filep, buffer, count)
Buffer is the address of count sequentially stored bytes (words in UNIX-7) which will be written onto the file. nwritten is the number of bytes actually written; except in rare cases it is the same as count. Occasionally, an error may be indicated; for example if paper tape is being written, an error occurs if the tape runs out.</p>

<p>For disk files which already existed (that is, were opened by open, not create) the bytes written affect only those implied by the position of the write pointer and the number of bytes written; no other part of the file is changed.</p>

<h5>Read</h5>

<p>To read, the call is</p>

<p>nread = read(filep, buffer, count)
Up to count bytes are read from the file into buffer. The number actually read is returned as nread. Every program must be prepared for the possibility that nread is less than count. If the read pointer is so near the end of the file that reading count characters would cause reading beyond the end, only sufficient bytes are transmitted to reach the end of the file. Furthermore, devices like the typewriters work in units of lines. Suppose, for example, that before anything has been typed a program tries to read 128 characters from the console. This forces the program to wait, since nothing has been typed. The user now types a line consisting, say, of 10 characters and hits the &quot;new line&quot; key. At this point the read call would return indicating 11 characters read (including the new line). On the other hand, it is permissible to read fewer characters than were typed without losing information; for example bytes may be picked up one at a time.</p>

<p>When the read call returns with nread equal to zero, it indicates the end of the file. For disk files this occurs when the read pointer becomes equal to the current size of the file. It is possible to generate an end-of-file from a typewriter by use of an escape sequence which depends on the device used.</p>

<h5>Seek</h5>

<p>To do &quot;random&quot;, that is, direct access I/O it is only necessary to move the read or write pointer to the appropriate location in the file.</p>

<p>seek(filep, base, offset)
The read pointer (respectively write pointer) associated with filep is moved to a position offset words from the beginning, from the current position of the pointer, or from the end of the file, depending on whether base is O, 1, or 2. Offset may he negative to move the pointer backwards. For some devices (e.g. paper tape and typewriters) seek calls are meaningless and are ignored.</p>

<h5>Tell</h5>

<p>The current position of the pointer may be discovered as follows:</p>

<p>offset = tell(filep, base)
As with seek, filep is the file descriptor for an open file, and base specifies whether the desired offset is to be measured from the beginning of the file, from the current position of the pointer, or from the end. In the second case, of course, the result is always zero.</p>

<p>Implementation of the File System
As mentioned in section 3.2 above, a directory entry contains only a name for the associated file and a pointer to the file itself. This pointer is an integer called the i-number (for identification number) of the file. When the file is accessed, its i-number is looked up in a system table stored in a known part of the disk. The entry thereby found (the file&#39;s i-node) contains the description of the file:</p>

<ol>
<li>its owner;</li>
<li>its protection bits;</li>
<li>the physical disk addresses for the file contents;</li>
<li>its size;</li>
<li>times of creation and last modification;</li>
<li>the number of links to the file; that is, the number of
 times it appears in a directory;</li>
<li>bits indicating whether the file is a directory and whether
 it is special (in which case the size and disk addresses
 are meaningless);</li>
<li>a bit indicating whether the file is &quot;large&quot; or &quot;small.&quot;
There is space in each i-node for eight disk addresses. A file which fits into eight or fewer 64-word (128-byte) blocks is considered small; in this case the addresses of the blocks themselves are stored. For large files, each of the eight disk addresses may point to an indirect block of 64 words containing the addresses of the blocks constituting the file itself. Thus files may be as large as 864128, or 65,536 bytes.</li>
</ol>

<p>When the number of links to a file drops to zero, its contents are freed and its i-node is marked unused.</p>

<p>To the user, both reading and writing of files appears to be synchronous and unbuffered. That is, immediately after return from a read call the data is available, and conversely after a write the user&#39;s workspace may be reused. In fact the system maintains, unseen by the user, a rather complicated buffering mechanism. Suppose a write call is made specifying transmission of a single byte. UNIX will search its own buffers to see whether the affected disk block currently resides in its own buffers; if not, it will be read in from the disk. Then the affected byte is replaced in the buffer and an entry is made in a list of blocks to be written on the disk. The return from the write call may then take place, although the actual I/O may not be completed until a later time. Conversely, if a single byte is read, the system determines whether the disk block in which the byte is located is already in one of the system&#39;s buffers; if so, the byte can be returned immediately. If not, the block is read into a buffer and the byte picked out. Because sequential reading of a file is so common, UNIX attempts to optimize this situation by prereading the disk block following the one in which the requested byte is found. This strategy tends to minimize and in some cases eliminate disk latency delays.</p>

<p>A program which reads or writes files in units of 128 bytes has an advantage over a program which reads or writes a single byte at a time, but the gain is not immense. As an example, the editor ed (8.9 and A2.4 below) was originally written, for simplicity, to do I/O one character at a time; it increased its speed by a factor of about two when it was rewritten to use 128-byte units. Because the system attempts to retain copies of the most recently used disk blocks in core, the speed gain in dealing with large units comes principally from elimination of system overhead, not from latency delays.</p>

	  ]]></description>
	</item>


</channel>
</rss>
