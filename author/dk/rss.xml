<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
   
      <title>hellodk.github.io/</title>
   
   <link>http://hellodk.github.io/</link>
   <description>A beautiful narrative written an elegant publishing platform. The story begins here.</description>
   <language>en-uk</language>
   <managingEditor> Deepak Kumar Gupta</managingEditor>
   <atom:link href="rss" rel="self" type="application/rss+xml" />
   
	<item>
	  <title>Linux tar commands examples</title>
	  <link>//linux_tar_examples</link>
	  <author>Deepak Kumar Gupta</author>
	  <pubDate>2016-02-23T19:13:00+05:30</pubDate>
	  <guid>//linux_tar_examples</guid>
	  <description><![CDATA[
	     <ol>
<li><p>Create a tar archive of a directory
In this example we will come to know basic tar command using option ‘cvf’ to create a tar archive.
Here we are creating a tar file ‘my-archive.tar’ for a directory ‘/home/my-directory’ in current working directory:
1
[root@chasiota /]# tar cvf my-archive.tar /home/my-directory
c – Creates a new .tar archive file
v – Verbosely list files which are processed
f – File name type of the archive file</p></li>
<li><p>Create a zipped archive
In order to make tar ball as zipped archive , we need to use the option “z”.
In below example the command will make “my-archive.tar.gz” for a directory “/home/my-directory” in current working directory.
1
[root@chasiota /]# tar cvzf my-archive.tar.gz /home/my-directory</p></li>
<li><p>List the contents of a tar archive
To list the contents of an uncompressed tar archive, just replace the cflag with the tflag, like this:
1
[root@chasiota /]# tar tvf my-archive.tar
This lists all the files in the archive, but does not extract them.
To list all the files in a compressed archive, add the zflag like before:
1
[root@chasiota /]# tar tzvf my-archive.tgz
or for a tar.gz archive:
1
[root@chasiota /]# tar tzvf my-archive.tar.gz</p></li>
<li><p>Extract tar archive contents
To extract the contents of a Linux tar archive, now just replace the tflag with the x(“extract”) flag. For uncompressed archives the extract command looks like this:
1
[root@chasiota /]# tar xvf my-archive.tar
For compressed archives the tar extract command looks like this:
1
[root@chasiota /]# tar xzvf my-archive.tar.gz
or this for *.tgz:
1
[root@chasiota /]# tar xzvf my-archive.tgz</p></li>
<li><p>Extract tar.bz2 archive contents
In order to extract the contents of a *.tar.bz2 file the options are “xj”.
1
[root@chasiota /]# tar xvjf my-archive.tar.bz2</p></li>
<li><p>Extract a single file from tar archive
To extract a specific file from a tar archive, specify the file name at the end of the tar xvf command as shown below. The following command extracts only a specific file (my-file.sh) in the current directory from a large tar file.
1
[root@chasiota /]# tar xvf my-archive.tar my-file.sh</p></li>
<li><p>Create a compressed archive of the current directory
Many times when using the Linux tar command you will want to create an archive of all files in the current directory, including all subdirectories. You can easily create this archive like this:
1
[root@chasiota /]# tar czvf my-directory.tgz .
In the above example, the ‘.’ at the end of the command is how you refer to the current directory.</p></li>
<li><p>Create an archive in a different directory
You may also want to create a new tar archive like that previous example in a different directory, like this:
1
[root@chasiota /]# tar czvf /tmp/my-directory.tgz .
As you can see, you just add a path before the name of your tar archive to specify what directory the archive should be created in.</p></li>
<li><p>Extract a single directory from tar archive
To extract a single directory (along with its subdirectory and files) from a tar archive, specify the directory name at the end of the tar xvf command as shown below. The following extracts only a specific directory from a large tar file:
1
[root@chasiota /]# tar xvf my-archive.tar home/my-directory/</p></li>
<li><p>Extract a single directory from tar.gz archive
We just need to add “z” to the above extract command “xvf”
1
[root@chasiota /]# tar xvzf my-archive.tar.gz home/my-directory/</p></li>
<li><p>Check the size of the tar, tar.gz and tar.bz2 Archive File
For any tar, tar.gz and tar.bz2 archive file, the below commands will display the size of archive file in Kilobytes (KB):
1
[root@chasiota /]# tar -cf - my-archive.tar | wc -l
1
[root@chasiota /]# tar -czf - my-archive.tar.gz | wc -l
1
[root@chasiota /]# tar -cjf  - my-archive.tar.bz2 | wc -l</p></li>
<li><p>Verify integrity of tar file
As part of creating a tar file, you can verify the integrity of the archive file that got created using the option “W” as shown below:
1
[root@chasiota /]# tar tvfW my-archive.tar
If an output line starts with Verify, and there is no differs line then the file/directory is OK. If not, you should investigate the issue.
Note: for a compressed archive file ( *.tar.gz, *.tar.bz2 ) you cannot do the verification.</p></li>
<li><p>Find the difference between an archive and file system
Finding the difference between an archive and file system can be done even for a compressed archive. It also shows the same output as above excluding the lines with Verify.
Finding the difference between gzip archive file and file system:
1
[root@chasiota /]# tar dfz my-archive.tgz
Finding the difference between bzip2 archive file and file system:
1
[root@chasiota /]# tar dfj my-archive.tar.bz2</p></li>
<li><p>Delete a file from tar ball
You can use the following syntax to delete a file from a tar ball:
1
[root@chasiota /]# tar --delete -f my-archive.tar home/my-file</p></li>
<li><p>Add a file to an existing archive
You can add additional files to an existing tar archive with “r” option:
1
[root@chasiota /]# tar rvf my-archive.tar my-file</p></li>
<li><p>Add a directory to an existing archive
Adding a directory is also the same. We need to mention the directory name instead of the file name:
1
[root@chasiota /]# tar rvf my-archive.tar my-dir/</p></li>
<li><p>Extract group of files from tar, tar.gz, tar.bz2 archives using regular expression
You can specify a regular expressions , to extract files matching a specified pattern. For example, the following tar command extracts all the files whose file ends with .java:
1
[root@chasiota /]# tar xvf my-archive.tar  --wildcards &#39;*.java&#39;</p></li>
<li><p>Untar multiple files from tar, tar.gz and tar.bz2 File
To extract or untar multiple files from the tar, tar.gz and tar.bz2 archive file. For example the below command will extract “my-file-1” “my-file-2” and “my-file-3” from the archive files:
1
[root@chasiota /]# tar -xvf my-archive.tar &quot;my-file-1&quot; &quot;my-file-2&quot; &quot;my-file-3&quot;
1
[root@chasiota /]# tar -zxvf my-archive.tar.gz &quot;my-file-1&quot; &quot;my-file-2&quot; &quot;my-file-3&quot;
1
[root@chasiota /]# tar -jxvf my-archive.tar.bz2 &quot;my-file-1&quot; &quot;my-file-2&quot; &quot;my-file-3&quot;</p></li>
<li><p>Restore files with tar
More important than performing regular backups is having them available when we need to recover important files. The following command will restore all files from the full-backup-Day-Month-Year.tar archive, which is an example backup of our home directory:
1
[root@chasiota /]# tar xpf /dev/st0/full-backup-Day-Month-Year.tar
The p option preserves permissions; file protection information will be remembered.</p></li>
<li><p>Check the manual page for tar
You can always refer to the manual page for all available tar commands:
1
[root@chasiota /]# man tar</p></li>
</ol>

	  ]]></description>
	</item>

	<item>
	  <title>The UNIX Time-Sharing System</title>
	  <link>//unix_time_sharing_system</link>
	  <author>Deepak Kumar Gupta</author>
	  <pubDate>2016-02-01T13:48:37+05:30</pubDate>
	  <guid>//unix_time_sharing_system</guid>
	  <description><![CDATA[
	     <div class="highlight"><pre><code class="language-text" data-lang="text">                          Drafted By

                        D. M. Ritchie
</code></pre></div>
<p>Introduction
UNIX is a general-purpose, multi-user time sharing system implemented on several Digital Equipment Corporation PDP series machines.</p>

<p>UNIX was written by K. L. Thompson, who also wrote many of the command programs. The author of this memorandum contributed several of the major commands, including the assembler and the debugger. The file system was originally designed by Thompson, the author, and R. H. Canaday.</p>

<p>There are two versions of UNIX. The first, which has been in existence about a year, runs on the PDP-7 and -9 computers; a more modern version, a few months old, uses the PDP-11. This document describes UNIX-11, since it is more modern and many of the differences between it and UNIX-7 result from redesign of features found to be deficient or lacking in the earlier system. Although the PDP-7 and PDP-11 are both small computers, the design of UNIX is amenable to expansion for use on more powerful machines. Indeed, UNIX contains a number of features very seldom offered even by larger systems, including</p>

<p>A versatile, convenient file system with complete integration between disk files and I/O devices;</p>

<p>The ability to initiate asynchrously running processes.</p>

<p>It must be said, however, that the most important features of UNIX are its simplicity, elegance, and ease of use.</p>

<p>Besides the system proper, the major programs available under UNIX are an assembler, a text editor based on QED, a symbolic debugger for examining and patching faulty programs, and &quot;B&quot;, a higher level language resembling BCPL. UNIX-7 also has a version of the compiler writing language TMGL contributed by M. D. McIlroy, and besides its own assembler, there is a PDP-11 assembler which was used to write UNIX-11. On the PDP-11 there is a version of BASIC [reference] adapted from the one supplied by DEC [reference]. All but the last of these programs were written locally, and except for the very first versions of the editor and assembler, using UNIX itself.</p>

<p>Hardware
The PDP-11 on which UNIX is implemented is a 16-bit 12K computer, and UNIX occupies 8K words. More than half of this space, however, is utilized for a variable number of disk buffers; with some loss of speed the number of buffers could be cut significantly.</p>

<p>The PDP-11 has a 256K word disk, almost all of which is used for file system storage. It is equipped with DECTAPE, a variety of magnetic tape facility in which individual records may be addressed and rewritten at will. Also available are a high-speed paper tape reader and punch. Besides the standard Teletype, there are several variable-speed communications interfaces.</p>

<p>The File System
The most important role of UNIX is to provide a file system. From the point of view of the user, there are three kinds of files: ordinary disk files, directories, and special files.</p>

<p>3.1 Ordinary Files</p>

<p>A file contains whatever information the user places there, for example symbolic or binary (object) programs. No particular structuring is expected by the system. Files of text ordinarily consist simply of a string of characters, with lines demarcated by the new-line character. Binary programs are sequences of words as they will appear in core memory when the program starts executing. A few user programs generate and expect files with more structure; for example, the assembler generates, and the debugger expects, a name list file in a particular format; however, the structure of files is controlled solely by the programs which use them, not by the system.</p>

<p>3.2 Directories</p>

<p>Directories (sometimes, &quot;catalogs&quot;), provide the mapping between the names of files and the files themselves, and thus induce a structure on the file system as a whole. Each user has a directory of his own files; he may also create subdirectories to contain groups of files conveniently treated together.</p>

<p>A directory is exactly like an ordinary file except that it cannot be written on by user programs, so that the system controls the contents of directories. However, anyone with appropriate permission may read a directory just like any other file.</p>

<p>The system maintains several directories for its own use. One of these is the root directory. All files in the system can be found by tracing a path through a chain of directories until the desired file is reached. The starting point for such searches is often the root, which contains an entry for each user&#39;s master directory. Another system directory contains all the programs provided as part of the system; that is, all the commands (elsewhere, &quot;subsystems&quot;). As will be seen, however, it is by no means necessary that a program reside in this directory for it to be used as a command.</p>

<p>Files and directories are named by sequences of eight or fewer characters. When the name of a file is specified to the system, it may be in the form of a path name, which is a sequence of directory names separated by slashes and ending in a file name. If the sequence begins with a slash, the search begins in the root directory. The name &quot;/a/b/c&quot; causes the system to search the root for directory &quot;a&quot;; then to search &quot;a&quot; for &quot;b&quot;, and then to find &quot;c&quot; in &quot;b&quot;. &quot;c&quot; may be an ordinary file, a directory, or a special file. As a limiting case, the name &quot;/&quot; refers to the root itself.</p>

<p>The same non-directory file may appear in several directories under possibly different names. This feature is called &quot;linking&quot;; a directory entry for a file is sometimes called a link. UNIX differs from other systems in which linking is permitted in that all links to a file have equal status. That is, a file does not exist within a particular directory; the directory entry for a file consists merely of its name and a pointer to the information actually describing the file. Thus a file exists independently of any directory entry, although in practice a file is made to disappear along with the last link to it.</p>

<p>When a user logs into UNIX, he is assigned a default current directory, but he may change to any directory readable by him. A path name not starting with &quot;/&quot; causes the system to begin the search in the userâ€™s current directory. Thus, the name &quot;a/b&quot; specifies the file named &quot;b&quot; in directory &quot;a&quot;, which is found in the current working directory. The simplest kind of name, for example &quot;a&quot;, refers to a file which itself is found in the working directory.</p>

<p>Each directory always has at least two entries. The name &quot;.&quot; in each directory refers to the directory itself. Thus a program may read the current directory under the name &quot;.&quot; without knowing its actual path name. The name &quot;..&quot; by convention refers to the parent of the directory in which it appears; that is, the directory in which it was first created.</p>

<p>The directory structure is constrained to have the form of a rooted tree. Except for the special entries &quot;.&quot; and &quot;..&quot;, each directory must appear as an entry in exactly one other, which is its parent. The reason for this is to simplify the writing of programs which visit subtrees of the directory structure, and more important, to avoid the separation of portions of the hierarchy. If arbitrary links to directories were permitted, it would be quite difficult to detect when the last connection from the root to a directory was severed.</p>

<p>3.3 Special Files</p>

<p>Special files constitute the most unusual feature of the UNIX file system. Each I/O device supported by UNIX is associated with at least one special file. Special files are read and written just like ordinary disk files, but the result is activation of the associated device. Entries for all special files reside in the root directory, so they may all be referred to by &quot;/&quot; followed by the appropriate name.</p>

<p>The special files are discussed further in section 6 below.</p>

<p>3.4 Protection</p>

<p>The protection scheme in UNIX is quite simple. Each user of the system is assigned a unique user number. When a file-is created, it is marked with the number of its creator. Also given for new files is a set of protection bits. Four of these specify independently permission to read or write for the owner of the file and for all other users. A fifth bit indicates permission to execute the file as a program. If the sixth bit is on, the system will temporarily change the user identification of the current user to that of the creator of the file whenever the file is executed as a program. This feature provides for privileged programs which may use files which should neither be read nor changed by other users. If the set-user-identification bit is on for a program, the accounting file may be accessed during the programâ€™s execution but not otherwise.</p>

<p>3.5 System I/O Calls</p>

<p>The system calls to do I/O are designed to eliminate the differences between the various devices and styles of access. There is no distinction between &quot;random&quot; and sequential I/O, nor is any logical or physical record size imposed by the system. The size of a file on the disk is determined by the location of the last piece of information written on it; no predetermination of the size of a file is necessary. In UNIX-11, the unit of information is the 8-bit byte, since the PDP-11 is a byte-oriented machine.</p>

<p>To illustrate the essentials of I/O in UNIX, the basic calls are summarized below in an anonymous higher level language which will indicate the needed parameters without getting into the complexities of machine language programming. (All system calls are also described in Appendix 1 in their actual form.) Each call to the system may potentially result in an error return, which for simplicity is not represented in the calling sequence.</p>

<p>3.5.1 Open</p>

<p>To read or write a file assumed to exist already, it must be Opened by the following call:</p>

<p>filep = open(name, flag)
Name indicates the name of the file. An arbitrary path name may be given. The flag argument indicates whether the file is to be read or written. If the file is to be &quot;updated&quot;, that is read and written simultaneously, it may be opened twice, once for reading and once for writing.</p>

<p>The returned argument filep is called a file descriptor. It is used to identify the file in subsequent calls to read, write or otherwise manipulate the file.</p>

<p>There are no locks in the file system, nor is there any restriction on the number of users who may have a file open for reading or writing. Although one may imagine situations in which this fact is unfortunate, in practice difficulties are quite rare.</p>

<p>3.5.2 Create</p>

<p>To create a new file, the following call is used.</p>

<p>filep = create(name, mode)
Here filep and name are as before. If the file already existed, it is truncated to zero length. Creation of a file implies opening for writing as well. The mode argument indicates the permissions which are to be placed on the file by the protection mechanism. To create a file, the user must have write permission in the directory in which the file is being created.</p>

<p>3.5.3 Write</p>

<p>Except as indicated below, reading and writing are sequential. This means that if a particular byte in the file was the last byte written (or read), the next I/O call implicitly refers to the first following byte. For each Open file there is a pointer, maintained by the system, which always indicates the next byte to be read or written. If n bytes are read, the pointer advances by n bytes.</p>

<p>Once a file is open for writing, the following call may be used.</p>

<p>nwritten = write(filep, buffer, count)
Buffer is the address of count sequentially stored bytes (words in UNIX-7) which will be written onto the file. nwritten is the number of bytes actually written; except in rare cases it is the same as count. Occasionally, an error may be indicated; for example if paper tape is being written, an error occurs if the tape runs out.</p>

<p>For disk files which already existed (that is, were opened by open, not create) the bytes written affect only those implied by the position of the write pointer and the number of bytes written; no other part of the file is changed.</p>

<p>3.5.4 Read</p>

<p>To read, the call is</p>

<p>nread = read(filep, buffer, count)
Up to count bytes are read from the file into buffer. The number actually read is returned as nread. Every program must be prepared for the possibility that nread is less than count. If the read pointer is so near the end of the file that reading count characters would cause reading beyond the end, only sufficient bytes are transmitted to reach the end of the file. Furthermore, devices like the typewriters work in units of lines. Suppose, for example, that before anything has been typed a program tries to read 128 characters from the console. This forces the program to wait, since nothing has been typed. The user now types a line consisting, say, of 10 characters and hits the &quot;new line&quot; key. At this point the read call would return indicating 11 characters read (including the new line). On the other hand, it is permissible to read fewer characters than were typed without losing information; for example bytes may be picked up one at a time.</p>

<p>When the read call returns with nread equal to zero, it indicates the end of the file. For disk files this occurs when the read pointer becomes equal to the current size of the file. It is possible to generate an end-of-file from a typewriter by use of an escape sequence which depends on the device used.</p>

<p>3.5.5 Seek</p>

<p>To do &quot;random&quot;, that is, direct access I/O it is only necessary to move the read or write pointer to the appropriate location in the file.</p>

<p>seek(filep, base, offset)
The read pointer (respectively write pointer) associated with filep is moved to a position offset words from the beginning, from the current position of the pointer, or from the end of the file, depending on whether base is O, 1, or 2. Offset may he negative to move the pointer backwards. For some devices (e.g. paper tape and typewriters) seek calls are meaningless and are ignored.</p>

<p>3.5.6 Tell</p>

<p>The current position of the pointer may be discovered as follows:</p>

<p>offset = tell(filep, base)
As with seek, filep is the file descriptor for an open file, and base specifies whether the desired offset is to be measured from the beginning of the file, from the current position of the pointer, or from the end. In the second case, of course, the result is always zero.</p>

<p>Implementation of the File System
As mentioned in section 3.2 above, a directory entry contains only a name for the associated file and a pointer to the file itself. This pointer is an integer called the i-number (for identification number) of the file. When the file is accessed, its i-number is looked up in a system table stored in a known part of the disk. The entry thereby found (the file&#39;s i-node) contains the description of the file:</p>

<ol>
<li>its owner;</li>
<li>its protection bits;</li>
<li>the physical disk addresses for the file contents;</li>
<li>its size;</li>
<li>times of creation and last modification;</li>
<li>the number of links to the file; that is, the number of
 times it appears in a directory;</li>
<li>bits indicating whether the file is a directory and whether
 it is special (in which case the size and disk addresses
 are meaningless);</li>
<li>a bit indicating whether the file is &quot;large&quot; or &quot;small.&quot;
There is space in each i-node for eight disk addresses. A file which fits into eight or fewer 64-word (128-byte) blocks is considered small; in this case the addresses of the blocks themselves are stored. For large files, each of the eight disk addresses may point to an indirect block of 64 words containing the addresses of the blocks constituting the file itself. Thus files may be as large as 864128, or 65,536 bytes.</li>
</ol>

<p>When the number of links to a file drops to zero, its contents are freed and its i-node is marked unused.</p>

<p>To the user, both reading and writing of files appears to be synchronous and unbuffered. That is, immediately after return from a read call the data is available, and conversely after a write the user&#39;s workspace may be reused. In fact the system maintains, unseen by the user, a rather complicated buffering mechanism. Suppose a write call is made specifying transmission of a single byte. UNIX will search its own buffers to see whether the affected disk block currently resides in its own buffers; if not, it will be read in from the disk. Then the affected byte is replaced in the buffer and an entry is made in a list of blocks to be written on the disk. The return from the write call may then take place, although the actual I/O may not be completed until a later time. Conversely, if a single byte is read, the system determines whether the disk block in which the byte is located is already in one of the system&#39;s buffers; if so, the byte can be returned immediately. If not, the block is read into a buffer and the byte picked out. Because sequential reading of a file is so common, UNIX attempts to optimize this situation by prereading the disk block following the one in which the requested byte is found. This strategy tends to minimize and in some cases eliminate disk latency delays.</p>

<p>A program which reads or writes files in units of 128 bytes has an advantage over a program which reads or writes a single byte at a time, but the gain is not immense. As an example, the editor ed (8.9 and A2.4 below) was originally written, for simplicity, to do I/O one character at a time; it increased its speed by a factor of about two when it was rewritten to use 128-byte units. Because the system attempts to retain copies of the most recently used disk blocks in core, the speed gain in dealing with large units comes principally from elimination of system overhead, not from latency delays.</p>

	  ]]></description>
	</item>

	<item>
	  <title>Markdown cheat-sheets</title>
	  <link>//mark_down_cheatsheet</link>
	  <author>Deepak Kumar Gupta</author>
	  <pubDate>2016-02-01T13:48:37+05:30</pubDate>
	  <guid>//mark_down_cheatsheet</guid>
	  <description><![CDATA[
	     <p>Source : https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet</p>

<h1>H1</h1>

<h2>H2</h2>

<h3>H3</h3>

<h4>H4</h4>

<h5>H5</h5>

<h6>H6</h6>

<h1>Will become a heading</h1>

<h2>Will become a sub heading</h2>

<p><em>This will be Italic</em></p>

<p><strong>This will be Bold</strong></p>

<ul>
<li>This will be a list item</li>
<li><p>This will be a list item</p>

<p>Add a indent and this will end up as code</p></li>
</ul>

<p>Alternatively, for H1 and H2, an underline-ish style:</p>

<h1>Alt-H1</h1>

<h2>Alt-H2</h2>

<p>Emphasis, aka italics, with <em>asterisks</em> or <em>underscores</em>.</p>

<p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p>

<p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p>

<p>Strikethrough uses two tildes. ~~Scratch this.~~</p>

<ol>
<li>First ordered list item</li>
<li>Another item
⋅⋅* Unordered sub-list. </li>
<li>Actual numbers don&#39;t matter, just that it&#39;s a number
⋅⋅1. Ordered sub-list</li>
<li>And another item.</li>
</ol>

<p>⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we&#39;ll use three here to also align the raw Markdown).</p>

<p>⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅
⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅
⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)</p>

<ul>
<li>Unordered list can use asterisks</li>
<li>Or minuses</li>
<li>Or pluses</li>
</ul>

<p><a href="https://www.google.com">I&#39;m an inline-style link</a></p>

<p><a href="https://www.google.com" title="Google&#39;s Homepage">I&#39;m an inline-style link with title</a></p>

<p><a href="https://www.mozilla.org">I&#39;m a reference-style link</a></p>

<p><a href="../blob/master/LICENSE">I&#39;m a relative reference to a repository file</a></p>

<p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p>

<p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p>

<p>URLs and URLs in angle brackets will automatically get turned into links. 
http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes 
example.com (but not on Github, for example).</p>

<p>Some text to show that the reference links can follow later.</p>

<p>Here&#39;s our logo (hover to see the title text):</p>

<p>Inline-style: 
<img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"></p>

<p>Reference-style: 
<img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2"></p>

<p>Inline <code>code</code> has <code>back-ticks around</code> it.</p>
<div class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="s2">&quot;JavaScript syntax highlighting&quot;</span><span class="p">;</span>
<span class="nx">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div><div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">s</span> <span class="o">=</span> <span class="s">&quot;Python syntax highlighting&quot;</span>
<span class="k">print</span> <span class="n">s</span>
</code></pre></div><div class="highlight"><pre><code class="language-text" data-lang="text">No language indicated, so no syntax highlighting. 
But let&#39;s throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div>
<p>var s = &quot;JavaScript syntax highlighting&quot;;
alert(s);</p>

<p>s = &quot;Python syntax highlighting&quot;
print s</p>

<p>No language indicated, so no syntax highlighting in Markdown Here (varies on Github). 
But let&#39;s throw in a <b>tag</b>.</p>

<p>Colons can be used to align columns.</p>

<p>| Tables        | Are           | Cool  |
| ------------- |:-------------:| -----:|
| col 3 is      | right-aligned | $1600 |
| col 2 is      | centered      |   $12 |
| zebra stripes | are neat      |    $1 |</p>

<p>There must be at least 3 dashes separating each header cell.
The outer pipes (|) are optional, and you don&#39;t need to make the 
raw Markdown line up prettily. You can also use inline Markdown.</p>

<p>Markdown | Less | Pretty
--- | --- | ---
<em>Still</em> | <code>renders</code> | <strong>nicely</strong>
1 | 2 | 3</p>

<blockquote>
<p>Blockquotes are very handy in email to emulate reply text.
This line is part of the same quote.</p>
</blockquote>

<p>Quote break.</p>

<blockquote>
<p>This is a very long line that will still be quoted properly when it wraps. Oh boy let&#39;s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote. </p>
</blockquote>

<dl>
  <dt>Definition list</dt>
  <dd>Is something people use sometimes.</dd>

  <dt>Markdown in HTML</dt>
  <dd>Does *not* work **very** well. Use HTML <em>tags</em>.</dd>
</dl>

<p>Three or more...</p>

<hr>

<p>Hyphens</p>

<hr>

<p>Asterisks</p>

<hr>

<p>Underscores</p>

<p>Here&#39;s a line for us to start with.</p>

<p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p>

<p>This line is also a separate paragraph, but...
This line is only separated by a single newline, so it&#39;s a separate line in the <em>same paragraph</em>.</p>

<p><a href="http://www.youtube.com/watch?feature=player_embedded&v=YOUTUBE_VIDEO_ID_HERE
" target="_blank"><img src="http://img.youtube.com/vi/YOUTUBE_VIDEO_ID_HERE/0.jpg" 
alt="IMAGE ALT TEXT HERE" width="240" height="180" border="10" /></a></p>

<p><a href="http://www.youtube.com/watch?v=YOUTUBE_VIDEO_ID_HERE"><img src="http://img.youtube.com/vi/YOUTUBE_VIDEO_ID_HERE/0.jpg" alt="IMAGE ALT TEXT HERE"></a></p>

	  ]]></description>
	</item>

	<item>
	  <title>Rethinkdb Installation on Ubuntu-14.04</title>
	  <link>//rethinkdb_installation_on%20_ubuntu-14.04</link>
	  <author>Deepak Kumar Gupta</author>
	  <pubDate>2016-01-28T15:48:00+05:30</pubDate>
	  <guid>//rethinkdb_installation_on%20_ubuntu-14.04</guid>
	  <description><![CDATA[
	     <p>Hey guys, hope you all are doing well. In this blog we are gonna see how do we setup rethinkdb server on an ubuntu box.</p>

<p>Before we get into the process of making our hands dirty, let&#39;s  first understand why rethinkdb? </p>

<p>Well, to say...rethinkdb is an open-source, scalable JSON database built from the ground up for the realtime web. RethinkDB inverts the traditional database architecture by exposing an exciting new access model – instead of polling for changes, the developer can tell RethinkDB to continuously push updated query results to applications in realtime. RethinkDB’s realtime push architecture dramatically reduces the time and effort necessary to build scalable realtime apps. RethinkDB also offers a flexible query language, intuitive operations and monitoring APIs, and is easy to setup and learn. Just like any other database solution, rethinkdb ships as a client-server component model. The installation process for both the server and the client are illustrated below:</p>

<p>How to Install RethinkDb:
1. Add the RethinkDB PPA to your list of repositories : source /etc/lsb-release &amp;&amp; echo &quot;deb http://download.rethinkdb.com/apt $DISTRIB_CODENAME main&quot; | sudo tee /etc/apt/sources.list.d/rethinkdb.list</p>

<ol>
<li>Add the keys:</li>
</ol>

<p>wget -qO- http://download.rethinkdb.com/apt/pubkey.gpg | sudo apt-key add -</p>

<ol>
<li>Update the repository:</li>
</ol>

<p>sudo apt-get update</p>

<ol>
<li>Install the rethinkdb server via apt-get:</li>
</ol>

<p>sudo apt-get -y install rethinkdb</p>

<p>Install rethinkdb client:</p>

<ol>
<li>Install the python-pip package:</li>
</ol>

<p>sudo apt-get install python-pip</p>

<ol>
<li>Install the rethinkdb python client:</li>
</ol>

<p>sudo pip install rethinkdb :</p>

<p>The above steps ensure that rethinkdb is installed on the system, while it does not ensures that this will start the rethinkdb service on system startup. You still need to start the rethinkdb service using the below command</p>

<p>The above command will ensure that rethinkdb is running as a terminal process, and will exit once the terminal is closed, or the process is killed, in short it will not run rethinkdb as a background service.</p>

<p>To start rethinkdb as a service, please follow the below steps:</p>

<ol>
<li><p>Go to /etc/rethinkdb and you will get the file, default.conf.sample</p></li>
<li><p>Copy the file to /etc/rethinkdb/instances.d and rename the file as per your requirements ensuring the extension is .conf only. Say for example the file name is rethinkdb1.conf</p></li>
<li><p>Now open the file /etc/rethinkdb/instances.d/rethinkdb1.conf and modify the paramaters as per your requirements.</p></li>
<li><p>If setting up a cluster, I suggest do change the server-name to somethink like &#39;rethinkdb-primary&#39; or &#39;rethinkdb-1&#39; or &#39;master&#39; or &#39;slave&#39;. This will ensure that we have a meaningful naming convention for our cluster.</p></li>
<li><p>The default port details are :</p></li>
</ol>

<p>29015 : Rethinkdb listens for intracluster connections</p>

<p>28015 : Rethinkdb listens for client driver connections</p>

<p>8080 : Rethinkdb listens for administrative HTTP connections</p>

<p>22 : For SSH. The server uses public key authentication.</p>

<p>80 : For HTTP. It is used during the setup process but otherwise redirects to HTTPS.</p>

<p>443 : For HTTPS. An Nginx server sits between RethinkDB and the world and provides basic HTTP authentication and secure HTTPS connections for the web UI</p>

	  ]]></description>
	</item>

	<item>
	  <title>Getting started with Cassandra</title>
	  <link>//getting_familiar_with_cassandra</link>
	  <author>Deepak Kumar Gupta</author>
	  <pubDate>2016-01-28T15:48:00+05:30</pubDate>
	  <guid>//getting_familiar_with_cassandra</guid>
	  <description><![CDATA[
	     <p>The Growth of Big Data</p>

<p>Big Data is one of the key forces driving the growth and popularity of NoSQL for business. The almost limitless array of data collection technologies ranging from simple online actions to point of sale systems to GPS tools to smartphones and tablets to sophisticated sensors – and many more – act as force multipliers for data growth.</p>

<p>In fact, one of the first reasons to use NoSQL is because you have a Big Data project to tackle. A Big Data project is normally typified by:</p>

<p>High data velocity – lots of data coming in very quickly, possibly from different locations.
Data variety – storage of data that is structured, semi-structured and unstructured.
Data volume – data that involves many terabytes or petabytes in size.
Data complexity – data that is stored and managed in different locations or data centers.</p>

<p>Datamodel   Performance Scalability Flexibility Complexity  Functionality
Key-value store High    High    High    None    Variable (None)
Column Store    High    High    Moderate    Low Minimal
Document Store  High    Variable (High) High    Low Variable (Low)
Graph Database  Variable    Variable    High    High    Graph Theory</p>

<p>massively scalable open source NoSQL database. Cassandra is perfect for managing large amounts of structured, semi-structured, and unstructured data across multiple data centers and the cloud. Cassandra delivers continuous availability, linear scalability, and operational simplicity across many commodity servers with no single point of failure, along with a powerful dynamic data model designed for maximum flexibility and fast response times</p>

<p>built-for-scale architecture means that it is capable of handling petabytes of information and thousands of concurrent users/operations per second.</p>

<p>An apache Software Foundation project, Cassandra is column oriented database and is an open source distributed database management system designed to handle large amounts of data across many commodity servers, providing high availability with no single point of failure.</p>

<p>Cassandra does not support joins or subqueries. Rather, Cassandra emphasizes denormalization through features like collections</p>

<p>CQL - Cassandara Query Language</p>

<p>Each node in a cluster can accept read and write requests, regardless of where the data is actually located in the cluster.</p>

<p>When a node goes down, read/write requests can be served from other nodes in the network.</p>

<p>The key components of Cassandra are as follows −
Node − It is the place where data is stored.
Data center − It is a collection of related nodes.
Cluster − A cluster is a component that contains one or more data centers.
Commit log − The commit log is a crash-recovery mechanism in Cassandra. Every write operation is written to the commit log.
Mem-table − A mem-table is a memory-resident data structure. After commit log, the data will be written to the mem-table. Sometimes, for a single-column family, there will be multiple mem-tables.
SSTable − It is a disk file to which the data is flushed from the mem-table when its contents reach a threshold value.
Bloom filter − These are nothing but quick, nondeterministic, algorithms for testing whether an element is a member of a set. It is a special kind of cache. Bloom filters are accessed after every query.</p>

<h2>Commands:</h2>

<p>nodetool cfstats : displays statistics for each table and keyspace.
nodetool cfhistograms : provides statistics about a table, including read/write latency, row size, column count, and number of SSTables.
nodetool netstats : provides statistics about network operations and connections.
nodetool tpstats : provides statistics about the number of active, pending, and completed tasks for each stage of Cassandra operations by thread pool.
nodetool status :</p>

<p>cqlsh <machine ip> -  connects to the machine cqlsh</p>

<h2>cqlsh command list:</h2>

<p>HELP - Displays help topics for all cqlsh commands.
CAPTURE - Captures the output of a command and adds it to a file.
CONSISTENCY - Shows the current consistency level, or sets a new consistency level.
COPY - Copies data to and from Cassandra.
DESCRIBE - Describes the current cluster of Cassandra and its objects.
EXPAND - Expands the output of a query vertically.
EXIT - Using this command, you can terminate cqlsh.
PAGING - Enables or disables query paging.
SHOW - Displays the details of current cqlsh session such as Cassandra version, host, or data type assumptions.
SOURCE - Executes a file that contains CQL statements.
TRACING - Enables or disables request tracing.</p>

<h2>Upgrading:</h2>

<p>mkdir ~/cassandra<em>backup
sudo cp -r /etc/cassandra/* ~/cassandra</em>backup
sudo vi /etc/cassandra/cassandra.yaml and edit num<em>tokens to 1 and uncomment the initial</em>token and set it to 1
nodetool upgradesstables
nodetool drain
sudo service cassandra stop
sudo cp -r /etc/cassandra/* ~/cassandra<em>backup</em>new
sudo apt-get install cassandra=2.1.12
Open the old and new cassandra.yaml files and diff them.
Merge the diffs by hand, including the partitioner setting, from the old file into the new one.
Do not use the default partitioner setting in the new cassandra.yaml because it has changed in this release to the Murmur3Partitioner. The Murmur3Partitioner can only be used for new clusters. After data has been added to the cluster, you cannot change the partitioner without reworking tables, which is not practical. Use your old partitioner setting in the new cassandra.yaml file.
Save the file as cassandra.yaml.</p>

<p>Configuration file &#39;/etc/cassandra/cassandra.yaml&#39;
 ==&gt; Modified (by you or by a script) since installation.
 ==&gt; Package distributor has shipped an updated version.
   What would you like to do about it ?  Your options are:
    Y or I  : install the package maintainer&#39;s version
    N or O  : keep your currently-installed version
      D     : show the differences between the versions
      Z     : start a shell to examine the situation
 The default action is to keep your current version.
*** cassandra.yaml (Y/I/N/O/D/Z) [default=N] ? </p>

<p>************<strong><em>Inserting values into tables</em></strong>**************************
CREATE KEYSPACE tutorialspoint WITH replication = {
  &#39;class&#39;: &#39;NetworkTopologyStrategy&#39;,
  &#39;cdr_record&#39;: &#39;2&#39;
};</p>

<p>INSERT INTO tutorialspoint.emp (emp<em>id,emp</em>city,emp<em>name,emp</em>phone,emp_sal) VALUES(3,&#39;Kolkata&#39;,&#39;Stag1&#39;,8791134412,60);</p>

<p>UPDATE TABLE emp(
   emp<em>id int PRIMARY KEY,
   emp</em>name text,
   emp<em>city text,
   emp</em>sal varint,
   emp_phone varint);</p>

<p>INSERT INTO TABLE emp(emp<em>id int,emp</em>name,emp<em>city,emp</em>sal,emp_phone) VALUES(1,&#39;dk&#39;,&#39;Bangalore&#39;,24,9964014500);</p>

<p>/usr/local/freeswitch/bin/fs_cli -x &quot;originate  freetdm/8/a/009716337516 &amp;park()&quot;</p>

<h2>Nodetool Command Set:</h2>

<p>nodetool status
nodetool info
nodetool -host 10.60.8.23 ring</p>

	  ]]></description>
	</item>

	<item>
	  <title>5 interesting cloud predictions for 2016</title>
	  <link>//5_interesting_cloud_predictions_2016</link>
	  <author>Deepak Kumar Gupta</author>
	  <pubDate>2016-01-28T15:48:00+05:30</pubDate>
	  <guid>//5_interesting_cloud_predictions_2016</guid>
	  <description><![CDATA[
	     <ol>
<li>It&#39;s time for IOT-</li>
</ol>

<p>The IOT or the Internet of Things has been a buzzword around for quite some time. And finally the time has come for IOT to be on boom. It is predicted that by the end of 2016, there will be one billion connected devices.</p>

<p>The Internet of Things is all set to harness the awesomeness of Cloud computing this year. IOT and Cloud combined together breaks free all limitations. The duo combo can help right from analyzing the weather conditions at your home and water the plants to conducting major surgeries remotely to powering drones for military, logistics etc. and what not!!</p>

<ol>
<li>Cloud is expanding - AWS coming to India in 2016. Owing to the huge demand in the Indian sub-continent for Cloud services, AWS(Amazon Web Services)- one of the top cloud services provider has plans to setup India region in 2016. Do I still need to say anything more on this?</li>
</ol>

<p>More and more startups will be focusing towards adapting cloud culture - Cloud is so versatile and flexible, it allows you to work from any corner of the world. Startups ideally do not have the infrastructure/resources to manage their own data-centers or hardware. Cloud provides then with Infrastructure as a Service at a very affordable rates, so they can focus more on their product.</p>

<ol>
<li><p>Outcast for more flexible cloud apps – The need for more flexible cloud apps can not be denied. With the rise in clod computing, will come the rise for ease of accessibility. This will trigger quiet a lot of cloud apps to outcast in the near future, similar to AWS CLI.</p></li>
<li><p>With the rise in better internet services and bandwidth in the second and third world&#39;s Dockerization/containerization will be emerging as a critical technology and on rise and will soon be a critical component in deployments.</p></li>
<li><p>Security - Cloud security should be a major concern for everyone working on cloud/IOT. One should perform a security assessment before starting their design. For IOT&#39;s, using an RTOS does not ensure security and neither does Encryption. One should ensure all attack vectors are addressed. Even if you are able to secure the cloud, rest assured it may not be enough because your device can still be compromised.</p></li>
</ol>

	  ]]></description>
	</item>

	<item>
	  <title>Understanding Git-Flow</title>
	  <link>//understanding_git_flow</link>
	  <author>Deepak Kumar Gupta</author>
	  <pubDate>2015-11-09T00:00:00+05:30</pubDate>
	  <guid>//understanding_git_flow</guid>
	  <description><![CDATA[
	     <p>Understanding Git Flow for(ubuntu-14.04):</p>

<p>what is git flow?
Usage?
How can it be helpful?</p>

<p>Install git flow using the below command:</p>

<p>sudo apt-get install git-flow</p>

<p>Starting with git flow:
To start with git flow, you first need to initialize it inside an existing repository. This is done by using the below command:</p>

<p>git flow init</p>

<p>In-case you want to force re-initialization of git flow, you can use the same command and pass -f as an argument
eg: git flow init -f</p>

<p>So far cool, this is dead simple, isn&#39;t it? Now let&#39;s see what happens once we do a git flow init. We&#39;ll have to answer a few questions regarding the naming conventions for our branches. For simplicity it is recommended to use the default values.</p>

<p>$ git flow init -f</p>

<p>Which branch should be used for bringing forth production releases?
   - develop
   - feature/lint_cleanup
   - master
Branch name for production releases: [master] </p>

<p>Which branch should be used for integration of the &quot;next release&quot;?
   - develop
   - feature/lint_cleanup
Branch name for &quot;next release&quot; development: [develop] </p>

<p>How to name your supporting branch prefixes?
Feature branches? [feature/] 
Release branches? [release/] 
Hotfix branches? [hotfix/] 
Support branches? [support/] 
Version tag prefix? [] </p>

<p>Creating/Deleting a tag:</p>

<p>Deleting a tag:</p>

<p>$ git tag -d <tag_name></p>

	  ]]></description>
	</item>

	<item>
	  <title>Going Agntless with Ansible</title>
	  <link>//going_agentless_with_ansible</link>
	  <author>Deepak Kumar Gupta</author>
	  <pubDate>2015-04-06T15:48:00+05:30</pubDate>
	  <guid>//going_agentless_with_ansible</guid>
	  <description><![CDATA[
	     <p>While looking for a light yet powerful Configuration Management tool, I was going through some of the various available tools, and came across Ansible. After having a quick look at the features, something which caught my attention: &quot;Ansible is agent-less&quot;, hence I decided to give it a try. Having used Ansible for some around more than 8+ months configuration management, I personally feel...Ansible is awesome!!
So what is Ansible? Well, it is a configuration management tool, and it empowers you to configure/orchestrate multiple systems at great ease, saving a lot of downtime and resources.
So how does it does that? The answer lies with ssh keys based agent forwarding. Ansible uses ssh for logging to the remote system, and can execute package installation, shell commands, install/update package management systems, clone git and anything under the sun you need for configuration management.
So why don&#39;t you try your hands on Ansible and experience it&#39;s awesomeness....all you need to do is to install it on your host system.</p>

<p>Ansible does not have it&#39;s own process, and unlike other configuration tolls available, it is agent-less!!</p>

<p>Ansible ships in 2 forms:
1. Core Ansible
2. Ansible Tower</p>

<p>Installation:
1. OS package manager:
sudo apt-get install software-properties-common
sudo apt-add-repository ppa:ansible/ansiblesudo apt-get update
sudo apt-get install ansible
2. Via python pip:
sudo pip install ansible</p>

<p>Ansible uses a configuration file, called host files for doing ssh to the remote machines. By default it is located in /etc/ansible/hosts
All you need to do is to add the host ip and the ssh_username to the file, and the public key of your host to the remote machine and you are all set to go!!</p>

<p>Commands:
ansible all -i ansible_hosts -m ping
ansible all -m ping -u deepak
ansible -m shell -a &#39;free -m&#39; host1
ansible all -m shell -a &#39;free -m&#39; -u dk</p>

<p>Playbooks are Ansible’s configuration, deployment, and orchestration language. They can describe a policy you want your remote systems to enforce, or a set of steps in a general IT process.</p>

<p>Playbooks are written in YAML language, and comprises of Plays</p>

<p>Running a Playbook in Ansible:
ansible-playbook playbook.yml -f 10</p>

	  ]]></description>
	</item>


</channel>
</rss>
